{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed mixed input - trial notebook\n",
    "\n",
    "1. Load model thru checkpoints, config & mint config utils\n",
    "    - Similar to `evaluator.py`\n",
    "1. Load tfrecord dataset as a starting point\n",
    "1. Mix up inputs in the dataset to create a new input (or new dataset, whichever is easier)\n",
    "1. Pass mixed input to model and see what it comes up with\n",
    "1. Prepend original input motion sequence and visualize prediction vs targets\n",
    "    - (targets being all the un-mixed inputs used to make the mixed input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: To use the exr data format, please install the OpenEXR package following the instructions detailed in the README at github.com/tensorflow/graphics.\n",
      "Warning: To use the threejs_vizualization, please install the colabtools package following the instructions detailed in the README at github.com/tensorflow/graphics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedWindow(verbose=True): could not load ipyvtklink try:\n",
      "> pip install ipyvtklink\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from mint.core import inputs\n",
    "from mint.core import model_builder\n",
    "from mint.ctl import single_task_evaluator\n",
    "from mint.utils import config_util\n",
    "from third_party.tf_models import orbit\n",
    "import tensorflow as tf\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import pprint\n",
    "import hashlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import copy\n",
    "import vedo\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from smplx import SMPL\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85207cf6fe4b401ea000478a6f258fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Path to config file:'), Text(value='./configs/motion_enc_pilot-audiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# layout helpers\n",
    "\n",
    "layout_path_input = widgets.Layout(width='700px', height='40px')\n",
    "\n",
    "# input widgets\n",
    "\n",
    "wg_config_path = widgets.Text(\n",
    "    value=\"./configs/motion_enc_pilot-audioseed-37_bsz8.config\",\n",
    "    placeholder=\"Path to config file\",\n",
    "    layout=layout_path_input,\n",
    ")\n",
    "\n",
    "wg_checkpoint_dir = widgets.Text(\n",
    "    value=\"/srv/share4/anarayanan68/mint/_expts/motion_enc_pilot_bsz8_1GPU/checkpoints\",\n",
    "    placeholder=\"Checkpoint directory to restore model from\",\n",
    "    layout=layout_path_input,\n",
    ")\n",
    "\n",
    "wg_enc_pkl_path = widgets.Text(\n",
    "    value=\"/srv/share4/anarayanan68/mint/_expts/motion_enc_pilot_bsz8_1GPU/enc_data.pkl\",\n",
    "    placeholder=\"Path to pkl file for motion name encoding\",\n",
    "    layout=layout_path_input,\n",
    ")\n",
    "\n",
    "wg_name_enc_yaml_path = widgets.Text(\n",
    "    value=\"./configs/joint_optim_pilot-name_enc.yml\",\n",
    "    placeholder=\"Path to name encoder YAML config file\",\n",
    "    layout=layout_path_input,\n",
    ")\n",
    "\n",
    "# overall container\n",
    "\n",
    "wg_container = widgets.VBox([\n",
    "    widgets.HBox([\n",
    "        widgets.Label(\"Path to config file:\"),\n",
    "        wg_config_path,\n",
    "    ]),\n",
    "    widgets.HBox([\n",
    "        widgets.Label(\"Checkpoint dir:\"),\n",
    "        wg_checkpoint_dir,\n",
    "    ]),\n",
    "    widgets.HBox([\n",
    "        widgets.Label(\"Path to encoding pkl file:\"),\n",
    "        wg_enc_pkl_path,\n",
    "    ]),\n",
    "    widgets.HBox([\n",
    "        widgets.Label(\"Path to name enc YAML config file:\"),\n",
    "        wg_name_enc_yaml_path,\n",
    "    ]),\n",
    "])\n",
    "\n",
    "display(wg_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./configs/pad_remove_test.config',\n",
       " './_expts/huber_loss_test/checkpoints/',\n",
       " './_expts/huber_loss_test/enc_data.pkl',\n",
       " './configs/pad_remove_test-name_enc.yml')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wg_config_path.value, wg_checkpoint_dir.value, wg_enc_pkl_path.value, wg_name_enc_yaml_path.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config read\n",
    "\n",
    "configs = config_util.get_configs_from_pipeline_file(wg_config_path.value)\n",
    "model_config = configs['model']\n",
    "eval_config = configs['eval_config']\n",
    "eval_dataset_config = configs['eval_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_enc_config_yaml = config_util.read_yaml_config(wg_name_enc_yaml_path.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored model from ./_expts/huber_loss_test/checkpoints/ckpt-29999.\n"
     ]
    }
   ],
   "source": [
    "# Model build & restore\n",
    "\n",
    "model = model_builder.build(model_config, is_training=False,\n",
    "    name_encoder_config_yaml=name_enc_config_yaml, dataset_config=eval_dataset_config)\n",
    "\n",
    "checkpoint_manager=tf.train.CheckpointManager(\n",
    "    tf.train.Checkpoint(model=model),\n",
    "    directory=wg_checkpoint_dir.value,\n",
    "    max_to_keep=None)\n",
    "\n",
    "checkpoint_path = checkpoint_manager.restore_or_initialize()\n",
    "\n",
    "if checkpoint_path is not None:\n",
    "    print(f\"restored model from {checkpoint_path}.\")\n",
    "else:\n",
    "    print(\"initialized model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original dataset\n",
    "orig_dataset = inputs.create_input(\n",
    "      train_eval_config=eval_config,\n",
    "      dataset_config=eval_dataset_config,\n",
    "      is_training=False,\n",
    "      use_tpu=False,\n",
    "      overfit_expt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['motion_name_enc', 'audio_name', 'audio_sequence_shape', 'motion_name', 'motion_sequence_shape', 'target', 'actual_motion_input', 'audio_input']\n",
      "{'actual_motion_input': <tf.Tensor: shape=(1, 120, 147), dtype=float32, numpy=\n",
      "array([[[-1.2844293e-01,  2.0173659e+00, -5.1968616e-01, ...,\n",
      "          4.7004530e-03,  9.9998093e-01, -4.0022093e-03],\n",
      "        [-1.2668689e-01,  2.0169802e+00, -5.1886302e-01, ...,\n",
      "          4.3621263e-03,  9.9998242e-01, -4.0218914e-03],\n",
      "        [-1.2377702e-01,  2.0160024e+00, -5.1727444e-01, ...,\n",
      "          3.9348225e-03,  9.9998450e-01, -3.9453711e-03],\n",
      "        ...,\n",
      "        [-6.8891376e-02,  1.9685647e+00, -4.8075879e-01, ...,\n",
      "          6.5236171e-03,  9.9997872e-01,  1.6711998e-06],\n",
      "        [-6.8966746e-02,  1.9688128e+00, -4.8220262e-01, ...,\n",
      "          6.5736515e-03,  9.9997836e-01,  7.3371804e-05],\n",
      "        [-6.7893244e-02,  1.9682292e+00, -4.8487023e-01, ...,\n",
      "          6.6540763e-03,  9.9997783e-01,  1.3623739e-04]]], dtype=float32)>,\n",
      " 'audio_input': <tf.Tensor: shape=(1, 240, 35), dtype=float32, numpy=\n",
      "array([[[   10.200293,   -78.86572 , -1321.3772  , ...,  -661.51404 ,\n",
      "         -1410.2347  ,  -918.63086 ],\n",
      "        [ -767.6909  ,  -279.41553 ,   141.13893 , ...,  -873.2308  ,\n",
      "         -1361.9302  ,  -766.9635  ],\n",
      "        [-1096.1177  ,  -986.4634  ,  -986.2331  , ...,  -505.15536 ,\n",
      "          -659.6232  ,  -197.16452 ],\n",
      "        ...,\n",
      "        [ -819.86597 ,   -32.27007 ,   179.36772 , ..., -1370.4993  ,\n",
      "           448.59976 , -1107.5026  ],\n",
      "        [   94.157845,    24.772533, -1440.4219  , ...,   -68.59414 ,\n",
      "          -418.644   ,   493.96088 ],\n",
      "        [ -186.7089  ,   297.06534 ,  -492.05557 , ...,  -434.20044 ,\n",
      "           149.46104 ,   296.94046 ]]], dtype=float32)>,\n",
      " 'audio_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA3'], dtype=object)>,\n",
      " 'audio_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[2095,   35]], dtype=int32)>,\n",
      " 'motion_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d26_mWA3_ch14'], dtype=object)>,\n",
      " 'motion_name_enc': <tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.]], dtype=float32)>,\n",
      " 'motion_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[2093,  147]], dtype=int32)>,\n",
      " 'target': <tf.Tensor: shape=(1, 60, 147), dtype=float32, numpy=\n",
      "array([[[-6.7614466e-02,  1.9675530e+00, -4.8510897e-01, ...,\n",
      "          6.6896281e-03,  9.9997759e-01,  1.1475342e-04],\n",
      "        [-6.7136154e-02,  1.9668260e+00, -4.8767421e-01, ...,\n",
      "          6.7735463e-03,  9.9997705e-01,  8.9852423e-05],\n",
      "        [-6.7387104e-02,  1.9665186e+00, -4.8997286e-01, ...,\n",
      "          6.5576755e-03,  9.9997848e-01,  1.7388233e-04],\n",
      "        ...,\n",
      "        [-6.1286848e-02,  1.9626979e+00, -4.9920508e-01, ...,\n",
      "          4.9053822e-03,  9.9998778e-01, -6.5206998e-04],\n",
      "        [-6.3005194e-02,  1.9632654e+00, -4.9756920e-01, ...,\n",
      "          4.6433620e-03,  9.9998897e-01, -7.0767925e-04],\n",
      "        [-6.1907694e-02,  1.9637871e+00, -4.9603966e-01, ...,\n",
      "          4.6875332e-03,  9.9998868e-01, -8.4166968e-04]]], dtype=float32)>}\n",
      "1 ['motion_name_enc', 'audio_name', 'audio_sequence_shape', 'motion_name', 'motion_sequence_shape', 'target', 'actual_motion_input', 'audio_input']\n",
      "{'actual_motion_input': <tf.Tensor: shape=(1, 120, 147), dtype=float32, numpy=\n",
      "array([[[ 1.2469971e-02,  1.9173514e+00, -4.0695569e-01, ...,\n",
      "          3.7566361e-03,  9.9999082e-01,  2.0511367e-03],\n",
      "        [ 1.1164918e-02,  1.9100534e+00, -4.1021171e-01, ...,\n",
      "          3.7214337e-03,  9.9999118e-01,  1.9471410e-03],\n",
      "        [ 9.9404193e-03,  1.8966630e+00, -4.1664764e-01, ...,\n",
      "          3.5546126e-03,  9.9999183e-01,  1.9379257e-03],\n",
      "        ...,\n",
      "        [ 2.0034105e-02,  1.9296935e+00, -3.8178241e-01, ...,\n",
      "          3.8449359e-03,  9.9999261e-01, -7.4669580e-05],\n",
      "        [ 2.0257711e-02,  1.9309661e+00, -3.8362846e-01, ...,\n",
      "          3.7359390e-03,  9.9999303e-01, -8.9333225e-05],\n",
      "        [ 1.9400025e-02,  1.9322009e+00, -3.8616052e-01, ...,\n",
      "          3.6421788e-03,  9.9999338e-01,  6.0395050e-05]]], dtype=float32)>,\n",
      " 'audio_input': <tf.Tensor: shape=(1, 240, 35), dtype=float32, numpy=\n",
      "array([[[  117.505455,    36.568863, -1092.5334  , ...,  -492.89874 ,\n",
      "         -1173.2806  ,  -726.5474  ],\n",
      "        [ -589.3844  ,  -145.67593 ,   236.4928  , ...,  -685.2912  ,\n",
      "         -1129.385   ,  -588.7234  ],\n",
      "        [ -887.8342  ,  -788.1886  ,  -787.9794  , ...,  -350.81155 ,\n",
      "          -491.1805  ,   -70.932304],\n",
      "        ...,\n",
      "        [ -636.79724 ,    78.911545,   271.23227 , ..., -1137.1719  ,\n",
      "           515.8904  ,  -898.18005 ],\n",
      "        [  193.79987 ,   130.74763 , -1200.7125  , ...,    45.90292 ,\n",
      "          -272.19647 ,   557.1112  ],\n",
      "        [  -61.431015,   378.18713 ,  -338.90744 , ...,  -286.33298 ,\n",
      "           244.0553  ,   378.07367 ]]], dtype=float32)>,\n",
      " 'audio_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA4'], dtype=object)>,\n",
      " 'audio_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1921,   35]], dtype=int32)>,\n",
      " 'motion_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d26_mWA4_ch12'], dtype=object)>,\n",
      " 'motion_name_enc': <tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0.]], dtype=float32)>,\n",
      " 'motion_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1919,  147]], dtype=int32)>,\n",
      " 'target': <tf.Tensor: shape=(1, 60, 147), dtype=float32, numpy=\n",
      "array([[[ 1.8261725e-02,  1.9309002e+00, -3.9008713e-01, ...,\n",
      "          3.7841350e-03,  9.9999285e-01, -5.4567245e-06],\n",
      "        [ 1.6378766e-02,  1.9283097e+00, -3.9343300e-01, ...,\n",
      "          4.1207606e-03,  9.9999148e-01,  1.0906350e-04],\n",
      "        [ 1.4488981e-02,  1.9256514e+00, -3.9535218e-01, ...,\n",
      "          4.4823438e-03,  9.9998993e-01,  1.3647429e-04],\n",
      "        ...,\n",
      "        [ 1.8769346e-02,  1.9302952e+00, -3.9331394e-01, ...,\n",
      "          4.5733410e-03,  9.9998945e-01,  4.6557465e-04],\n",
      "        [ 1.8677143e-02,  1.9276097e+00, -3.9619380e-01, ...,\n",
      "          4.4015017e-03,  9.9999017e-01,  5.1250437e-04],\n",
      "        [ 1.8917963e-02,  1.9243463e+00, -3.9897910e-01, ...,\n",
      "          4.6350788e-03,  9.9998921e-01,  3.6443427e-04]]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "# see samples in dataset\n",
    "for i,x in enumerate(orig_dataset):\n",
    "    if i >= 2:\n",
    "        break\n",
    "    print(i, list(x.keys()))\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each entry in the `orig_dataset` is the `inputs.py`-preprocessed dict of each `tfexample` written by `tools/preprocessing.py`**\n",
    "<br>\n",
    "So to make a new input, just create a dict that can be passed to the model just as `SingleTaskEvaluator` passes its `inputs`. No `tf` dataset necessary.\n",
    "<br>\n",
    "But, to compute the input, use some inputs from the `orig_dataset` and also the encoding scheme from `tools/preprocessing.py`. Also don't forget to use the actual motion inputs etc from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fns from preprocessing, for motion name encoding\n",
    "\n",
    "from tools import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'onehot_enc_map': {'len': 20,\n",
       "  'gWA_sFM_cAll_d25_mWA4_ch05': {'index': 0,\n",
       "   'enc': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d25_mWA2_ch03': {'index': 1,\n",
       "   'enc': array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d27_mWA2_ch21': {'index': 2,\n",
       "   'enc': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d25_mWA5_ch07': {'index': 3,\n",
       "   'enc': array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d27_mWA2_ch17': {'index': 4,\n",
       "   'enc': array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d25_mWA5_ch06': {'index': 5,\n",
       "   'enc': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d26_mWA5_ch13': {'index': 6,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d25_mWA1_ch02': {'index': 7,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d27_mWA3_ch18': {'index': 8,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d26_mWA1_ch09': {'index': 9,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d27_mWA4_ch19': {'index': 10,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d26_mWA3_ch14': {'index': 11,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d27_mWA5_ch20': {'index': 12,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d25_mWA3_ch04': {'index': 13,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d26_mWA4_ch12': {'index': 14,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d26_mWA3_ch11': {'index': 15,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gLH_sBM_cAll_d18_mLH2_ch06': {'index': 16,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0.])},\n",
       "  'gLH_sBM_cAll_d17_mLH0_ch09': {'index': 17,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 0., 0.])},\n",
       "  'gLH_sBM_cAll_d17_mLH1_ch09': {'index': 18,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 0.])},\n",
       "  'gLH_sBM_cAll_d16_mLH2_ch04': {'index': 19,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 1.])}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_pkl_data = preprocessing.load_enc_pkl(wg_enc_pkl_path.value)\n",
    "enc_pkl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gWA_sFM_cAll_d27_mWA2_ch17', 'gWA_sFM_cAll_d27_mWA4_ch19')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_motion_name_1 = \"gWA_sFM_cAll_d27_mWA2_ch17\"\n",
    "target_motion_name_2 = \"gWA_sFM_cAll_d27_mWA4_ch19\"\n",
    "\n",
    "target_motion_name_1, target_motion_name_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motion_name_from_example(example):\n",
    "    return bytes.decode(example['motion_name'].numpy().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 60, 147])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA2'], dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d27_mWA2_ch17'], dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 60, 147])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA4'], dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d27_mWA4_ch19'], dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inp1, inp2 = None, None\n",
    "for inp in orig_dataset:\n",
    "    name = motion_name_from_example(inp)\n",
    "    if name == target_motion_name_1:\n",
    "        inp1 = inp\n",
    "    elif name == target_motion_name_2:\n",
    "        inp2 = inp\n",
    "\n",
    "    if inp1 is not None and inp2 is not None:\n",
    "        break\n",
    "\n",
    "display(inp1['target'].shape)\n",
    "display(inp1['audio_name'], inp1['motion_name'])\n",
    "display(inp2['target'].shape)\n",
    "display(inp2['audio_name'], inp2['motion_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: passing actual input and saving output\n",
    "op_inp1 = model(inp1)\n",
    "np.save(\"./_expts/op_inp1.npy\", op_inp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_inp2 = model(inp2)\n",
    "np.save(\"./_expts/op_inp2.npy\", op_inp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expt 1: Midpt *after* encoding** - NOT POSSIBLE (would need to modify the model to do this now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expt 2: Midpt *before* encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gWA_sFM_cAll_d27_mWA2_ch17'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'gWA_sFM_cAll_d27_mWA4_ch19'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name1, name2 = motion_name_from_example(inp1), motion_name_from_example(inp2)\n",
    "display(name1, name2, type(name1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_latent_1, name_latent_2 = (\n",
    "    preprocessing.get_latent_from_seq_name(name1, enc_pkl_data),\n",
    "    preprocessing.get_latent_from_seq_name(name2, enc_pkl_data)\n",
    ")\n",
    "name_latent_1, name_latent_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0. , 0. , 0. , 0.5, 0. , 0. , 0. , 0. , 0. , 0.5, 0. , 0. ,\n",
       "       0. , 0. , 0. , 0. , 0. , 0. , 0. ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_latent = 0.5 * (name_latent_1 + name_latent_2)\n",
    "mix_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_inp__before_enc = copy.deepcopy(inp1)\n",
    "mix_inp__before_enc['motion_name_enc'] = mix_latent[None, ...]  # add batch dim\n",
    "\n",
    "op = model(mix_inp__before_enc)\n",
    "np.save(\"./_expts/mix_op__before_enc.npy\", op)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73ef9dae1850d4826a6a2abacc613ceaa38b06453232522e74b0afa9475d9130"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('ai-choreo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
