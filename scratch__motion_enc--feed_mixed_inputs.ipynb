{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed mixed input - trial notebook\n",
    "\n",
    "1. Load model thru checkpoints, config & mint config utils\n",
    "    - Similar to `evaluator.py`\n",
    "1. Load tfrecord dataset as a starting point\n",
    "1. Mix up inputs in the dataset to create a new input (or new dataset, whichever is easier)\n",
    "1. Pass mixed input to model and see what it comes up with\n",
    "1. Prepend original input motion sequence and visualize prediction vs targets\n",
    "    - (targets being all the un-mixed inputs used to make the mixed input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: To use the exr data format, please install the OpenEXR package following the instructions detailed in the README at github.com/tensorflow/graphics.\n",
      "Warning: To use the threejs_vizualization, please install the colabtools package following the instructions detailed in the README at github.com/tensorflow/graphics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedWindow(verbose=True): could not load ipyvtklink try:\n",
      "> pip install ipyvtklink\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from mint.core import inputs\n",
    "from mint.core import model_builder\n",
    "from mint.ctl import single_task_evaluator\n",
    "from mint.utils import config_util\n",
    "from third_party.tf_models import orbit\n",
    "import tensorflow as tf\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import pprint\n",
    "import hashlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import copy\n",
    "import vedo\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from smplx import SMPL\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06200c1127e40758383d26da76b175b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Path to config file:'), Text(value='./configs/motion_enc_pilot-audiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# layout helpers\n",
    "\n",
    "layout_path_input = widgets.Layout(width='700px', height='40px')\n",
    "\n",
    "# input widgets\n",
    "\n",
    "wg_config_path = widgets.Text(\n",
    "    value=\"./configs/motion_enc_pilot-audioseed-37_bsz8.config\",\n",
    "    placeholder=\"Path to config file\",\n",
    "    layout=layout_path_input,\n",
    ")\n",
    "\n",
    "wg_checkpoint_dir = widgets.Text(\n",
    "    value=\"/srv/share4/anarayanan68/mint/_expts/motion_enc_pilot_bsz8_1GPU/checkpoints\",\n",
    "    placeholder=\"Checkpoint directory to restore model from\",\n",
    "    layout=layout_path_input,\n",
    ")\n",
    "\n",
    "wg_enc_pkl_path = widgets.Text(\n",
    "    value=\"/srv/share4/anarayanan68/mint/_expts/motion_enc_pilot_bsz8_1GPU/enc_data.pkl\",\n",
    "    placeholder=\"Path to pkl file for motion name encoding\",\n",
    "    layout=layout_path_input,\n",
    ")\n",
    "\n",
    "wg_name_enc_yaml_path = widgets.Text(\n",
    "    value=\"./configs/joint_optim_pilot-name_enc.yml\",\n",
    "    placeholder=\"Path to name encoder YAML config file\",\n",
    "    layout=layout_path_input,\n",
    ")\n",
    "\n",
    "# overall container\n",
    "\n",
    "wg_container = widgets.VBox([\n",
    "    widgets.HBox([\n",
    "        widgets.Label(\"Path to config file:\"),\n",
    "        wg_config_path,\n",
    "    ]),\n",
    "    widgets.HBox([\n",
    "        widgets.Label(\"Checkpoint dir:\"),\n",
    "        wg_checkpoint_dir,\n",
    "    ]),\n",
    "    widgets.HBox([\n",
    "        widgets.Label(\"Path to encoding pkl file:\"),\n",
    "        wg_enc_pkl_path,\n",
    "    ]),\n",
    "    widgets.HBox([\n",
    "        widgets.Label(\"Path to name enc YAML config file:\"),\n",
    "        wg_name_enc_yaml_path,\n",
    "    ]),\n",
    "])\n",
    "\n",
    "display(wg_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('configs/rotation_6d_pilot-audioseed37_bsz8.config',\n",
       " '_expts/rotation_6d_pilot_bsz8_1GPU/checkpoints',\n",
       " '_expts/rotation_6d_pilot_bsz8_1GPU/enc_data.pkl',\n",
       " 'configs/rotation_6d_pilot-name_enc.yml')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wg_config_path.value, wg_checkpoint_dir.value, wg_enc_pkl_path.value, wg_name_enc_yaml_path.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config read\n",
    "\n",
    "configs = config_util.get_configs_from_pipeline_file(wg_config_path.value)\n",
    "model_config = configs['model']\n",
    "eval_config = configs['eval_config']\n",
    "eval_dataset_config = configs['eval_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_enc_config_yaml = config_util.read_yaml_config(wg_name_enc_yaml_path.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored model from _expts/rotation_6d_pilot_bsz8_1GPU/checkpoints/ckpt-29999.\n"
     ]
    }
   ],
   "source": [
    "# Model build & restore\n",
    "\n",
    "model = model_builder.build(model_config, is_training=False,\n",
    "    name_encoder_config_yaml=name_enc_config_yaml, dataset_config=eval_dataset_config)\n",
    "\n",
    "checkpoint_manager=tf.train.CheckpointManager(\n",
    "    tf.train.Checkpoint(model=model),\n",
    "    directory=wg_checkpoint_dir.value,\n",
    "    max_to_keep=None)\n",
    "\n",
    "checkpoint_path = checkpoint_manager.restore_or_initialize()\n",
    "\n",
    "if checkpoint_path is not None:\n",
    "    print(f\"restored model from {checkpoint_path}.\")\n",
    "else:\n",
    "    print(\"initialized model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original dataset\n",
    "orig_dataset = inputs.create_input(\n",
    "      train_eval_config=eval_config,\n",
    "      dataset_config=eval_dataset_config,\n",
    "      is_training=False,\n",
    "      use_tpu=False,\n",
    "      overfit_expt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['motion_name_enc', 'audio_name', 'audio_sequence_shape', 'motion_name', 'motion_sequence_shape', 'target', 'actual_motion_input', 'audio_input']\n",
      "{'actual_motion_input': <tf.Tensor: shape=(1, 120, 153), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          1.7886530e-03,  9.9999338e-01,  3.1724998e-03],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          1.3982248e-03,  9.9999326e-01,  3.3959723e-03],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          1.1130214e-03,  9.9999219e-01,  3.7853713e-03],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          4.4081826e-03,  9.9999017e-01, -5.0006859e-04],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          4.4498751e-03,  9.9998999e-01, -4.9670704e-04],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          4.6262066e-03,  9.9998915e-01, -5.6008791e-04]]], dtype=float32)>,\n",
      " 'audio_input': <tf.Tensor: shape=(1, 240, 35), dtype=float32, numpy=\n",
      "array([[[  117.505455,    36.568863, -1092.5334  , ...,  -492.89874 ,\n",
      "         -1173.2806  ,  -726.5474  ],\n",
      "        [ -589.3844  ,  -145.67593 ,   236.4928  , ...,  -685.2912  ,\n",
      "         -1129.385   ,  -588.7234  ],\n",
      "        [ -887.8342  ,  -788.1886  ,  -787.9794  , ...,  -350.81155 ,\n",
      "          -491.1805  ,   -70.932304],\n",
      "        ...,\n",
      "        [ -636.79724 ,    78.911545,   271.23227 , ..., -1137.1719  ,\n",
      "           515.8904  ,  -898.18005 ],\n",
      "        [  193.79987 ,   130.74763 , -1200.7125  , ...,    45.90292 ,\n",
      "          -272.19647 ,   557.1112  ],\n",
      "        [  -61.431015,   378.18713 ,  -338.90744 , ...,  -286.33298 ,\n",
      "           244.0553  ,   378.07367 ]]], dtype=float32)>,\n",
      " 'audio_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA4'], dtype=object)>,\n",
      " 'audio_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1921,   35]], dtype=int32)>,\n",
      " 'motion_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d25_mWA4_ch05'], dtype=object)>,\n",
      " 'motion_name_enc': <tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.]], dtype=float32)>,\n",
      " 'motion_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1919,  147]], dtype=int32)>,\n",
      " 'target': <tf.Tensor: shape=(1, 60, 153), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          4.6690316e-03,  9.9998897e-01, -4.7027873e-04],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          4.6246918e-03,  9.9998915e-01, -5.5800698e-04],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          4.7647497e-03,  9.9998838e-01, -7.0130441e-04],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          8.3695538e-04,  9.9999726e-01,  2.2002803e-03],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          8.8122039e-04,  9.9999720e-01,  2.1925371e-03],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          8.2759204e-04,  9.9999750e-01,  2.0768296e-03]]], dtype=float32)>}\n",
      "1 ['motion_name_enc', 'audio_name', 'audio_sequence_shape', 'motion_name', 'motion_sequence_shape', 'target', 'actual_motion_input', 'audio_input']\n",
      "{'actual_motion_input': <tf.Tensor: shape=(1, 120, 153), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          1.3655049e-03,  9.9999815e-01, -1.3704748e-03],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          1.4117054e-03,  9.9999821e-01, -1.2646650e-03],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          1.6448422e-03,  9.9999803e-01, -1.0936825e-03],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          2.8793118e-03,  9.9999565e-01,  6.2755519e-04],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          2.8183269e-03,  9.9999601e-01,  2.2254851e-04],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          2.8360377e-03,  9.9999595e-01,  7.3155774e-05]]], dtype=float32)>,\n",
      " 'audio_input': <tf.Tensor: shape=(1, 240, 35), dtype=float32, numpy=\n",
      "array([[[ 228.6918   ,  177.93639  , -530.1248   , ..., -154.09326  ,\n",
      "         -580.7614   , -300.6146   ],\n",
      "        [-214.59952  ,   63.650505 ,  303.30887  , ..., -274.74274  ,\n",
      "         -553.23444  , -214.185    ],\n",
      "        [-401.7577   , -339.2698   , -339.13858  , ...,  -64.99025  ,\n",
      "         -153.01575  ,  110.52231  ],\n",
      "        ...,\n",
      "        [-244.33214  ,  204.48953  ,  325.09406  , ..., -558.11755  ,\n",
      "          478.51938  , -408.24554  ],\n",
      "        [ 276.5361   ,  236.99599  , -597.9639   , ...,  183.78978  ,\n",
      "          -15.6906395,  504.36902  ],\n",
      "        [ 116.480576 ,  392.16553  ,  -57.525166 , ...,  -24.555668 ,\n",
      "          308.05133  ,  392.0944   ]]], dtype=float32)>,\n",
      " 'audio_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA5'], dtype=object)>,\n",
      " 'audio_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1773,   35]], dtype=int32)>,\n",
      " 'motion_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d25_mWA5_ch06'], dtype=object)>,\n",
      " 'motion_name_enc': <tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.]], dtype=float32)>,\n",
      " 'motion_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1771,  147]], dtype=int32)>,\n",
      " 'target': <tf.Tensor: shape=(1, 60, 153), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          2.6078869e-03,  9.9999660e-01,  3.4358243e-05],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          2.0714074e-03,  9.9999785e-01,  6.2018180e-06],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          1.5971740e-03,  9.9999875e-01, -1.0046869e-05],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          9.2649070e-04,  9.9999708e-01,  2.2402441e-03],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          1.3988510e-03,  9.9999648e-01,  2.2520246e-03],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          1.5712611e-03,  9.9999636e-01,  2.1850537e-03]]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "# see samples in dataset\n",
    "for i,x in enumerate(orig_dataset):\n",
    "    if i >= 2:\n",
    "        break\n",
    "    print(i, list(x.keys()))\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each entry in the `orig_dataset` is the `inputs.py`-preprocessed dict of each `tfexample` written by `tools/preprocessing.py`**\n",
    "<br>\n",
    "So to make a new input, just create a dict that can be passed to the model just as `SingleTaskEvaluator` passes its `inputs`. No `tf` dataset necessary.\n",
    "<br>\n",
    "But, to compute the input, use some inputs from the `orig_dataset` and also the encoding scheme from `tools/preprocessing.py`. Also don't forget to use the actual motion inputs etc from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fns from preprocessing, for motion name encoding\n",
    "\n",
    "from tools import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'onehot_enc_map': {'len': 20,\n",
       "  'gWA_sFM_cAll_d25_mWA4_ch05': {'index': 0,\n",
       "   'enc': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d25_mWA2_ch03': {'index': 1,\n",
       "   'enc': array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d27_mWA2_ch21': {'index': 2,\n",
       "   'enc': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d25_mWA5_ch07': {'index': 3,\n",
       "   'enc': array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d27_mWA2_ch17': {'index': 4,\n",
       "   'enc': array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d25_mWA5_ch06': {'index': 5,\n",
       "   'enc': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d26_mWA5_ch13': {'index': 6,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d25_mWA1_ch02': {'index': 7,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d27_mWA3_ch18': {'index': 8,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d26_mWA1_ch09': {'index': 9,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d27_mWA4_ch19': {'index': 10,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d26_mWA3_ch14': {'index': 11,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d27_mWA5_ch20': {'index': 12,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d25_mWA3_ch04': {'index': 13,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d26_mWA4_ch12': {'index': 14,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d26_mWA3_ch11': {'index': 15,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gLH_sBM_cAll_d18_mLH2_ch06': {'index': 16,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0.])},\n",
       "  'gLH_sBM_cAll_d17_mLH0_ch09': {'index': 17,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 0., 0.])},\n",
       "  'gLH_sBM_cAll_d17_mLH1_ch09': {'index': 18,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 0.])},\n",
       "  'gLH_sBM_cAll_d16_mLH2_ch04': {'index': 19,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 1.])}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_pkl_data = preprocessing.load_enc_pkl(wg_enc_pkl_path.value)\n",
    "enc_pkl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gWA_sFM_cAll_d27_mWA2_ch17', 'gWA_sFM_cAll_d27_mWA4_ch19')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_motion_name_1 = \"gWA_sFM_cAll_d27_mWA2_ch17\"\n",
    "target_motion_name_2 = \"gWA_sFM_cAll_d27_mWA4_ch19\"\n",
    "\n",
    "target_motion_name_1, target_motion_name_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motion_name_from_example(example):\n",
    "    return bytes.decode(example['motion_name'].numpy().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 60, 153])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA2'], dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d27_mWA2_ch17'], dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 60, 153])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA4'], dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d27_mWA4_ch19'], dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inp1, inp2 = None, None\n",
    "for inp in orig_dataset:\n",
    "    name = motion_name_from_example(inp)\n",
    "    if name == target_motion_name_1:\n",
    "        inp1 = inp\n",
    "    elif name == target_motion_name_2:\n",
    "        inp2 = inp\n",
    "\n",
    "    if inp1 is not None and inp2 is not None:\n",
    "        break\n",
    "\n",
    "display(inp1['target'].shape)\n",
    "display(inp1['audio_name'], inp1['motion_name'])\n",
    "display(inp2['target'].shape)\n",
    "display(inp2['audio_name'], inp2['motion_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: passing actual input and saving output\n",
    "op_inp1 = model(inp1)\n",
    "np.save(\"./_expts/op_inp1.npy\", op_inp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_inp2 = model(inp2)\n",
    "np.save(\"./_expts/op_inp2.npy\", op_inp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expt 1: Midpt *after* encoding** - NOT POSSIBLE (would need to modify the model to do this now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expt 2: Midpt *before* encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gWA_sFM_cAll_d27_mWA2_ch17'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'gWA_sFM_cAll_d27_mWA4_ch19'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name1, name2 = motion_name_from_example(inp1), motion_name_from_example(inp2)\n",
    "display(name1, name2, type(name1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_latent_1, name_latent_2 = (\n",
    "    preprocessing.get_latent_from_seq_name(name1, enc_pkl_data),\n",
    "    preprocessing.get_latent_from_seq_name(name2, enc_pkl_data)\n",
    ")\n",
    "name_latent_1, name_latent_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0. , 0. , 0. , 0.5, 0. , 0. , 0. , 0. , 0. , 0.5, 0. , 0. ,\n",
       "       0. , 0. , 0. , 0. , 0. , 0. , 0. ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_latent = 0.5 * (name_latent_1 + name_latent_2)\n",
    "mix_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_inp__before_enc = copy.deepcopy(inp1)\n",
    "mix_inp__before_enc['motion_name_enc'] = mix_latent[None, ...]  # add batch dim\n",
    "\n",
    "op = model(mix_inp__before_enc)\n",
    "np.save(\"./_expts/mix_op__before_enc.npy\", op)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73ef9dae1850d4826a6a2abacc613ceaa38b06453232522e74b0afa9475d9130"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('ai-choreo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
