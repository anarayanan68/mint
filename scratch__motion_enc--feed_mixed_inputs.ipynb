{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed mixed input - trial notebook\n",
    "\n",
    "1. Load model thru checkpoints, config & mint config utils\n",
    "    - Similar to `evaluator.py`\n",
    "1. Load tfrecord dataset as a starting point\n",
    "1. Mix up inputs in the dataset to create a new input (or new dataset, whichever is easier)\n",
    "1. Pass mixed input to model and see what it comes up with\n",
    "1. Prepend original input motion sequence and visualize prediction vs targets\n",
    "    - (targets being all the un-mixed inputs used to make the mixed input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: To use the exr data format, please install the OpenEXR package following the instructions detailed in the README at github.com/tensorflow/graphics.\n",
      "Warning: To use the threejs_vizualization, please install the colabtools package following the instructions detailed in the README at github.com/tensorflow/graphics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedWindow(verbose=True): could not load ipyvtklink try:\n",
      "> pip install ipyvtklink\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from mint.core import inputs\n",
    "from mint.core import model_builder\n",
    "from mint.ctl import single_task_evaluator\n",
    "from mint.utils import config_util\n",
    "from third_party.tf_models import orbit\n",
    "import tensorflow as tf\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import pprint\n",
    "import hashlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import copy\n",
    "import vedo\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from smplx import SMPL\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac9ab1a7f744a428a6fc10762cc59e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Path to config file:'), Text(value='./configs/motion_enc_pilot-audiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# layout helpers\n",
    "\n",
    "layout_path_input = widgets.Layout(width='700px', height='40px')\n",
    "\n",
    "# input widgets\n",
    "\n",
    "wg_config_path = widgets.Text(\n",
    "    value=\"./configs/motion_enc_pilot-audioseed-37_bsz8.config\",\n",
    "    placeholder=\"Path to config file\",\n",
    "    layout=layout_path_input,\n",
    ")\n",
    "\n",
    "wg_checkpoint_dir = widgets.Text(\n",
    "    value=\"/srv/share4/anarayanan68/mint/_expts/motion_enc_pilot_bsz8_1GPU/checkpoints\",\n",
    "    placeholder=\"Checkpoint directory to restore model from\",\n",
    "    layout=layout_path_input,\n",
    ")\n",
    "\n",
    "wg_enc_pkl_path = widgets.Text(\n",
    "    value=\"/srv/share4/anarayanan68/mint/_expts/motion_enc_pilot_bsz8_1GPU/enc_data.pkl\",\n",
    "    placeholder=\"Path to pkl file for motion name encoding\",\n",
    "    layout=layout_path_input,\n",
    ")\n",
    "\n",
    "\n",
    "# overall container\n",
    "\n",
    "wg_container = widgets.VBox([\n",
    "    widgets.HBox([\n",
    "        widgets.Label(\"Path to config file:\"),\n",
    "        wg_config_path,\n",
    "    ]),\n",
    "    widgets.HBox([\n",
    "        widgets.Label(\"Checkpoint dir:\"),\n",
    "        wg_checkpoint_dir,\n",
    "    ]),\n",
    "    widgets.HBox([\n",
    "        widgets.Label(\"Path to encoding pkl file:\"),\n",
    "        wg_enc_pkl_path,\n",
    "    ]),\n",
    "])\n",
    "\n",
    "display(wg_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./configs/motion_enc_pilot-audioseed-37_bsz8.config',\n",
       " '/coc/scratch/anarayanan68/mint/_expts/tvloss_overfit_l2_bsz8_1GPU/checkpoints',\n",
       " '/srv/share4/anarayanan68/mint/_expts/motion_enc_pilot_bsz8_1GPU/enc_data.pkl')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wg_config_path.value, wg_checkpoint_dir.value, wg_enc_pkl_path.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config read\n",
    "\n",
    "configs = config_util.get_configs_from_pipeline_file(wg_config_path.value)\n",
    "model_config = configs['model']\n",
    "eval_config = configs['eval_config']\n",
    "eval_dataset_config = configs['eval_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored model from /coc/scratch/anarayanan68/mint/_expts/tvloss_overfit_l2_bsz8_1GPU/checkpoints/ckpt-29999.\n"
     ]
    }
   ],
   "source": [
    "# Model build & restore\n",
    "\n",
    "model = model_builder.build(model_config, is_training=False)   # even using True would work as the arg is unused\n",
    "\n",
    "checkpoint_manager=tf.train.CheckpointManager(\n",
    "    tf.train.Checkpoint(model=model),\n",
    "    directory=wg_checkpoint_dir.value,\n",
    "    max_to_keep=None)\n",
    "\n",
    "checkpoint_path = checkpoint_manager.restore_or_initialize()\n",
    "\n",
    "if checkpoint_path is not None:\n",
    "    print(f\"restored model from {checkpoint_path}.\")\n",
    "else:\n",
    "    print(\"initialized model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original dataset\n",
    "orig_dataset = inputs.create_input(\n",
    "      train_eval_config=eval_config,\n",
    "      dataset_config=eval_dataset_config,\n",
    "      is_training=False,\n",
    "      use_tpu=False,\n",
    "      overfit_expt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['audio_name', 'audio_sequence_shape', 'motion_name', 'motion_name_enc_shape', 'motion_sequence_shape', 'motion_input', 'target', 'actual_motion_input', 'audio_input']\n",
      "{'actual_motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          1.6797760e-04, -1.5142176e-03,  9.9999887e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          2.5011573e-04, -1.5279740e-03,  9.9999881e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          3.1027434e-04, -1.4991260e-03,  9.9999881e-01],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -3.1654290e-03,  1.6448519e-03,  9.9999362e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -3.3644640e-03,  1.7190618e-03,  9.9999285e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -3.1808640e-03,  1.8863098e-03,  9.9999315e-01]]], dtype=float32)>,\n",
      " 'audio_input': <tf.Tensor: shape=(1, 240, 35), dtype=float32, numpy=\n",
      "array([[[  -66.404236,  -187.17056 , -1871.9159  , ...,  -977.1947  ,\n",
      "         -1992.3997  , -1325.8243  ],\n",
      "        [-1121.162   ,  -459.0999  ,   111.13799 , ..., -1264.2655  ,\n",
      "         -1926.9025  , -1120.1757  ],\n",
      "        [-1566.482   , -1417.7998  , -1417.4875  , ...,  -765.18494 ,\n",
      "          -974.6309  ,  -347.5742  ],\n",
      "        ...,\n",
      "        [-1191.9071  ,  -123.990616,   162.97314 , ..., -1938.5214  ,\n",
      "           528.03    , -1581.9191  ],\n",
      "        [   47.435432,   -46.645454, -2033.3309  , ...,  -173.24313 ,\n",
      "          -647.8825  ,   589.5361  ],\n",
      "        [ -333.39722 ,   322.5616  ,  -747.42267 , ...,  -668.97577 ,\n",
      "           122.4221  ,   322.39227 ]]], dtype=float32)>,\n",
      " 'audio_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA2'], dtype=object)>,\n",
      " 'audio_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[2305,   35]], dtype=int32)>,\n",
      " 'motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[  0.        ,   0.        ,   0.        , ...,  20.153263  ,\n",
      "          -0.167489  ,   6.5832696 ],\n",
      "        [  0.        ,   0.        ,   0.        , ...,   2.6014333 ,\n",
      "          -5.6509733 ,  -2.3196948 ],\n",
      "        [  0.        ,   0.        ,   0.        , ...,  11.652421  ,\n",
      "         -10.716403  ,   2.3934188 ],\n",
      "        ...,\n",
      "        [  0.        ,   0.        ,   0.        , ...,   0.08122845,\n",
      "           6.9026394 , -10.650259  ],\n",
      "        [  0.        ,   0.        ,   0.        , ...,   4.1686287 ,\n",
      "           2.1609128 ,  -6.126605  ],\n",
      "        [  0.        ,   0.        ,   0.        , ...,  -9.804112  ,\n",
      "          -9.0738735 ,  15.1948    ]]], dtype=float32)>,\n",
      " 'motion_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d27_mWA2_ch17'], dtype=object)>,\n",
      " 'motion_name_enc_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[256, 219]], dtype=int32)>,\n",
      " 'motion_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[2302,  219]], dtype=int32)>,\n",
      " 'target': <tf.Tensor: shape=(1, 60, 225), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -2.9761654e-03,  1.8887573e-03,  9.9999380e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -3.2305168e-03,  1.8676411e-03,  9.9999303e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -3.3108173e-03,  1.9226546e-03,  9.9999267e-01],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -2.2740943e-04, -1.6856255e-03,  9.9999857e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -1.7343610e-04, -1.6156591e-03,  9.9999869e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -2.2339600e-04, -1.6392363e-03,  9.9999863e-01]]], dtype=float32)>}\n",
      "1 ['audio_name', 'audio_sequence_shape', 'motion_name', 'motion_name_enc_shape', 'motion_sequence_shape', 'motion_input', 'target', 'actual_motion_input', 'audio_input']\n",
      "{'actual_motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -1.4138616e-03, -4.8556848e-04,  9.9999887e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -1.4673109e-03, -4.4936457e-04,  9.9999881e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -1.3489244e-03, -3.8303231e-04,  9.9999905e-01],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -7.1387948e-04,  4.3646788e-04,  9.9999964e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -1.4102898e-03,  7.3639827e-04,  9.9999875e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -1.2916403e-03,  9.9289301e-04,  9.9999869e-01]]], dtype=float32)>,\n",
      " 'audio_input': <tf.Tensor: shape=(1, 240, 35), dtype=float32, numpy=\n",
      "array([[[  117.505455,    36.568863, -1092.5334  , ...,  -492.89874 ,\n",
      "         -1173.2806  ,  -726.5474  ],\n",
      "        [ -589.3844  ,  -145.67593 ,   236.4928  , ...,  -685.2912  ,\n",
      "         -1129.385   ,  -588.7234  ],\n",
      "        [ -887.8342  ,  -788.1886  ,  -787.9794  , ...,  -350.81155 ,\n",
      "          -491.1805  ,   -70.932304],\n",
      "        ...,\n",
      "        [ -636.79724 ,    78.911545,   271.23227 , ..., -1137.1719  ,\n",
      "           515.8904  ,  -898.18005 ],\n",
      "        [  193.79987 ,   130.74763 , -1200.7125  , ...,    45.90292 ,\n",
      "          -272.19647 ,   557.1112  ],\n",
      "        [  -61.431015,   378.18713 ,  -338.90744 , ...,  -286.33298 ,\n",
      "           244.0553  ,   378.07367 ]]], dtype=float32)>,\n",
      " 'audio_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA4'], dtype=object)>,\n",
      " 'audio_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1921,   35]], dtype=int32)>,\n",
      " 'motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[ 0.        ,  0.        ,  0.        , ...,  9.425348  ,\n",
      "         -7.32774   ,  2.2107084 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ...,  1.51284   ,\n",
      "          6.92795   ,  1.9468949 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., 11.550777  ,\n",
      "          0.20376764,  0.79299265],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        , ...,  4.272851  ,\n",
      "         -2.6133904 , -7.737772  ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -1.4917573 ,\n",
      "          5.564007  ,  3.2351894 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ...,  3.51044   ,\n",
      "         -1.5938603 , 22.315084  ]]], dtype=float32)>,\n",
      " 'motion_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d27_mWA4_ch19'], dtype=object)>,\n",
      " 'motion_name_enc_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[256, 219]], dtype=int32)>,\n",
      " 'motion_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1919,  219]], dtype=int32)>,\n",
      " 'target': <tf.Tensor: shape=(1, 60, 225), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -7.9008820e-04,  1.0378842e-03,  9.9999917e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -6.1219448e-04,  1.1633387e-03,  9.9999917e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -5.0080783e-04,  1.1743933e-03,  9.9999917e-01],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          6.1659620e-04, -5.0144461e-03,  9.9998724e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          6.6436664e-04, -4.8395987e-03,  9.9998808e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          6.6722982e-04, -4.8291832e-03,  9.9998814e-01]]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "# see samples in dataset\n",
    "for i,x in enumerate(orig_dataset):\n",
    "    if i >= 2:\n",
    "        break\n",
    "    print(i, list(x.keys()))\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each entry in the `orig_dataset` is the `inputs.py`-preprocessed dict of each `tfexample` written by `tools/preprocessing.py`**\n",
    "<br>\n",
    "So to make a new input, just create a dict that can be passed to the model just as `SingleTaskEvaluator` passes its `inputs`. No `tf` dataset necessary.\n",
    "<br>\n",
    "But, to compute the input, use some inputs from the `orig_dataset` and also the encoding scheme from `tools/preprocessing.py`. Also don't forget to use the actual motion inputs etc from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fns from preprocessing, for motion name encoding\n",
    "\n",
    "def compute_hashed_name(seq_name):\n",
    "    hash_str = hashlib.sha1(seq_name.encode('utf-8')).hexdigest()\n",
    "    stride = 4\n",
    "    hash_np = np.array([\n",
    "        float(int(hash_str[i:i+stride], 16)) / 16**stride\n",
    "        for i in range(0, len(hash_str), stride)\n",
    "    ]).reshape((1, -1))\n",
    "    return hash_np\n",
    "\n",
    "\n",
    "def load_enc_pkl(pkl_path):\n",
    "    res = None\n",
    "    if pkl_path is not None and os.path.exists(pkl_path):\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            res = pickle.load(f)\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_encoded_hash(hash_np, enc_pkl_data):\n",
    "    w1 = enc_pkl_data['w1']\n",
    "    b1 = enc_pkl_data['b1']\n",
    "    w2 = enc_pkl_data['w2']\n",
    "    b2 = enc_pkl_data['b2']\n",
    "    enc_shape = enc_pkl_data['enc_shape']\n",
    "\n",
    "    z1 = hash_np @ w1 + b1\n",
    "    op = np.tanh(z1) @ w2 + b2\n",
    "\n",
    "    return op.reshape(enc_shape)\n",
    "\n",
    "\n",
    "def get_encoded_input(seq_name, enc_pkl_data):\n",
    "    hash_np = compute_hashed_name(seq_name)\n",
    "    return get_encoded_hash(hash_np, enc_pkl_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_pkl_data = load_enc_pkl(wg_enc_pkl_path.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'actual_motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          1.6797760e-04, -1.5142176e-03,  9.9999887e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          2.5011573e-04, -1.5279740e-03,  9.9999881e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          3.1027434e-04, -1.4991260e-03,  9.9999881e-01],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -3.1654290e-03,  1.6448519e-03,  9.9999362e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -3.3644640e-03,  1.7190618e-03,  9.9999285e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -3.1808640e-03,  1.8863098e-03,  9.9999315e-01]]], dtype=float32)>,\n",
      " 'audio_input': <tf.Tensor: shape=(1, 240, 35), dtype=float32, numpy=\n",
      "array([[[  -66.404236,  -187.17056 , -1871.9159  , ...,  -977.1947  ,\n",
      "         -1992.3997  , -1325.8243  ],\n",
      "        [-1121.162   ,  -459.0999  ,   111.13799 , ..., -1264.2655  ,\n",
      "         -1926.9025  , -1120.1757  ],\n",
      "        [-1566.482   , -1417.7998  , -1417.4875  , ...,  -765.18494 ,\n",
      "          -974.6309  ,  -347.5742  ],\n",
      "        ...,\n",
      "        [-1191.9071  ,  -123.990616,   162.97314 , ..., -1938.5214  ,\n",
      "           528.03    , -1581.9191  ],\n",
      "        [   47.435432,   -46.645454, -2033.3309  , ...,  -173.24313 ,\n",
      "          -647.8825  ,   589.5361  ],\n",
      "        [ -333.39722 ,   322.5616  ,  -747.42267 , ...,  -668.97577 ,\n",
      "           122.4221  ,   322.39227 ]]], dtype=float32)>,\n",
      " 'audio_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA2'], dtype=object)>,\n",
      " 'audio_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[2305,   35]], dtype=int32)>,\n",
      " 'motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[  0.        ,   0.        ,   0.        , ...,  20.153263  ,\n",
      "          -0.167489  ,   6.5832696 ],\n",
      "        [  0.        ,   0.        ,   0.        , ...,   2.6014333 ,\n",
      "          -5.6509733 ,  -2.3196948 ],\n",
      "        [  0.        ,   0.        ,   0.        , ...,  11.652421  ,\n",
      "         -10.716403  ,   2.3934188 ],\n",
      "        ...,\n",
      "        [  0.        ,   0.        ,   0.        , ...,   0.08122845,\n",
      "           6.9026394 , -10.650259  ],\n",
      "        [  0.        ,   0.        ,   0.        , ...,   4.1686287 ,\n",
      "           2.1609128 ,  -6.126605  ],\n",
      "        [  0.        ,   0.        ,   0.        , ...,  -9.804112  ,\n",
      "          -9.0738735 ,  15.1948    ]]], dtype=float32)>,\n",
      " 'motion_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d27_mWA2_ch17'], dtype=object)>,\n",
      " 'motion_name_enc_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[256, 219]], dtype=int32)>,\n",
      " 'motion_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[2302,  219]], dtype=int32)>,\n",
      " 'target': <tf.Tensor: shape=(1, 60, 225), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -2.9761654e-03,  1.8887573e-03,  9.9999380e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -3.2305168e-03,  1.8676411e-03,  9.9999303e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -3.3108173e-03,  1.9226546e-03,  9.9999267e-01],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -2.2740943e-04, -1.6856255e-03,  9.9999857e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -1.7343610e-04, -1.6156591e-03,  9.9999869e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -2.2339600e-04, -1.6392363e-03,  9.9999863e-01]]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "it = iter(orig_dataset)\n",
    "inp1 = next(it)\n",
    "inp2 = next(it)\n",
    "\n",
    "pprint.pprint(inp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'actual_motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -1.4138616e-03, -4.8556848e-04,  9.9999887e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -1.4673109e-03, -4.4936457e-04,  9.9999881e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -1.3489244e-03, -3.8303231e-04,  9.9999905e-01],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -7.1387948e-04,  4.3646788e-04,  9.9999964e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -1.4102898e-03,  7.3639827e-04,  9.9999875e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -1.2916403e-03,  9.9289301e-04,  9.9999869e-01]]], dtype=float32)>,\n",
      " 'audio_input': <tf.Tensor: shape=(1, 240, 35), dtype=float32, numpy=\n",
      "array([[[  117.505455,    36.568863, -1092.5334  , ...,  -492.89874 ,\n",
      "         -1173.2806  ,  -726.5474  ],\n",
      "        [ -589.3844  ,  -145.67593 ,   236.4928  , ...,  -685.2912  ,\n",
      "         -1129.385   ,  -588.7234  ],\n",
      "        [ -887.8342  ,  -788.1886  ,  -787.9794  , ...,  -350.81155 ,\n",
      "          -491.1805  ,   -70.932304],\n",
      "        ...,\n",
      "        [ -636.79724 ,    78.911545,   271.23227 , ..., -1137.1719  ,\n",
      "           515.8904  ,  -898.18005 ],\n",
      "        [  193.79987 ,   130.74763 , -1200.7125  , ...,    45.90292 ,\n",
      "          -272.19647 ,   557.1112  ],\n",
      "        [  -61.431015,   378.18713 ,  -338.90744 , ...,  -286.33298 ,\n",
      "           244.0553  ,   378.07367 ]]], dtype=float32)>,\n",
      " 'audio_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA4'], dtype=object)>,\n",
      " 'audio_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1921,   35]], dtype=int32)>,\n",
      " 'motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[ 0.        ,  0.        ,  0.        , ...,  9.425348  ,\n",
      "         -7.32774   ,  2.2107084 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ...,  1.51284   ,\n",
      "          6.92795   ,  1.9468949 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., 11.550777  ,\n",
      "          0.20376764,  0.79299265],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        , ...,  4.272851  ,\n",
      "         -2.6133904 , -7.737772  ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -1.4917573 ,\n",
      "          5.564007  ,  3.2351894 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ...,  3.51044   ,\n",
      "         -1.5938603 , 22.315084  ]]], dtype=float32)>,\n",
      " 'motion_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d27_mWA4_ch19'], dtype=object)>,\n",
      " 'motion_name_enc_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[256, 219]], dtype=int32)>,\n",
      " 'motion_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1919,  219]], dtype=int32)>,\n",
      " 'target': <tf.Tensor: shape=(1, 60, 225), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -7.9008820e-04,  1.0378842e-03,  9.9999917e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -6.1219448e-04,  1.1633387e-03,  9.9999917e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -5.0080783e-04,  1.1743933e-03,  9.9999917e-01],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          6.1659620e-04, -5.0144461e-03,  9.9998724e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          6.6436664e-04, -4.8395987e-03,  9.9998808e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          6.6722982e-04, -4.8291832e-03,  9.9998814e-01]]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(inp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: passing actual input and saving output\n",
    "op_inp1 = model(inp1)\n",
    "np.save(\"./_expts/op_inp1.npy\", op_inp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_inp2 = model(inp2)\n",
    "np.save(\"./_expts/op_inp2.npy\", op_inp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 60, 225])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA2'], dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d27_mWA2_ch17'], dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(inp1['target'].shape)\n",
    "display(inp1['audio_name'], inp1['motion_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 60, 225])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA4'], dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d27_mWA4_ch19'], dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(inp2['target'].shape)\n",
    "display(inp2['audio_name'], inp2['motion_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expt 1: Midpt *after* encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
       "array([[[ 0.        ,  0.        ,  0.        , ..., 14.789306  ,\n",
       "         -3.7476146 ,  4.396989  ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  2.0571365 ,\n",
       "          0.6384883 , -0.18639994],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., 11.601599  ,\n",
       "         -5.2563176 ,  1.5932057 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  2.1770396 ,\n",
       "          2.1446245 , -9.1940155 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  1.3384356 ,\n",
       "          3.8624597 , -1.4457078 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -3.1468363 ,\n",
       "         -5.333867  , 18.754942  ]]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_inp = 0.5 * (inp1['motion_input'] + inp2['motion_input'])\n",
    "motion_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['audio_name',\n",
       " 'audio_sequence_shape',\n",
       " 'motion_name',\n",
       " 'motion_name_enc_shape',\n",
       " 'motion_sequence_shape',\n",
       " 'motion_input',\n",
       " 'target',\n",
       " 'actual_motion_input',\n",
       " 'audio_input']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(inp1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'actual_motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          1.6797760e-04, -1.5142176e-03,  9.9999887e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          2.5011573e-04, -1.5279740e-03,  9.9999881e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          3.1027434e-04, -1.4991260e-03,  9.9999881e-01],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -3.1654290e-03,  1.6448519e-03,  9.9999362e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -3.3644640e-03,  1.7190618e-03,  9.9999285e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -3.1808640e-03,  1.8863098e-03,  9.9999315e-01]]], dtype=float32)>,\n",
      " 'audio_input': <tf.Tensor: shape=(1, 240, 35), dtype=float32, numpy=\n",
      "array([[[  -66.404236,  -187.17056 , -1871.9159  , ...,  -977.1947  ,\n",
      "         -1992.3997  , -1325.8243  ],\n",
      "        [-1121.162   ,  -459.0999  ,   111.13799 , ..., -1264.2655  ,\n",
      "         -1926.9025  , -1120.1757  ],\n",
      "        [-1566.482   , -1417.7998  , -1417.4875  , ...,  -765.18494 ,\n",
      "          -974.6309  ,  -347.5742  ],\n",
      "        ...,\n",
      "        [-1191.9071  ,  -123.990616,   162.97314 , ..., -1938.5214  ,\n",
      "           528.03    , -1581.9191  ],\n",
      "        [   47.435432,   -46.645454, -2033.3309  , ...,  -173.24313 ,\n",
      "          -647.8825  ,   589.5361  ],\n",
      "        [ -333.39722 ,   322.5616  ,  -747.42267 , ...,  -668.97577 ,\n",
      "           122.4221  ,   322.39227 ]]], dtype=float32)>,\n",
      " 'audio_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA2'], dtype=object)>,\n",
      " 'audio_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[2305,   35]], dtype=int32)>,\n",
      " 'motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[ 0.        ,  0.        ,  0.        , ..., 14.789306  ,\n",
      "         -3.7476146 ,  4.396989  ],\n",
      "        [ 0.        ,  0.        ,  0.        , ...,  2.0571365 ,\n",
      "          0.6384883 , -0.18639994],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., 11.601599  ,\n",
      "         -5.2563176 ,  1.5932057 ],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        , ...,  2.1770396 ,\n",
      "          2.1446245 , -9.1940155 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ...,  1.3384356 ,\n",
      "          3.8624597 , -1.4457078 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -3.1468363 ,\n",
      "         -5.333867  , 18.754942  ]]], dtype=float32)>,\n",
      " 'motion_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d27_mWA2_ch17'], dtype=object)>,\n",
      " 'motion_name_enc_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[256, 219]], dtype=int32)>,\n",
      " 'motion_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[2302,  219]], dtype=int32)>,\n",
      " 'target': <tf.Tensor: shape=(1, 60, 225), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -2.9761654e-03,  1.8887573e-03,  9.9999380e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -3.2305168e-03,  1.8676411e-03,  9.9999303e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -3.3108173e-03,  1.9226546e-03,  9.9999267e-01],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -2.2740943e-04, -1.6856255e-03,  9.9999857e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -1.7343610e-04, -1.6156591e-03,  9.9999869e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -2.2339600e-04, -1.6392363e-03,  9.9999863e-01]]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "mix_inp__after_enc = copy.deepcopy(inp1)\n",
    "mix_inp__after_enc['motion_input'] = motion_inp\n",
    "\n",
    "pprint.pprint(mix_inp__after_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Tensor: shape=(1, 360, 225), dtype=float32, numpy=\n",
      "array([[[-1.8552907e-01,  6.1986886e-04, -8.9536384e-02, ...,\n",
      "         -1.4867255e-01,  1.5773547e-01,  7.6246375e-01],\n",
      "        [ 1.8293066e-01, -4.5286570e-04,  2.8150442e-01, ...,\n",
      "          9.9786207e-02, -3.6589332e-02,  1.0474453e+00],\n",
      "        [ 1.5402207e-02,  8.7621577e-02,  8.8924766e-02, ...,\n",
      "         -1.1416654e-01,  2.9354692e-01,  8.7109554e-01],\n",
      "        ...,\n",
      "        [-7.6220062e+01, -1.2719684e+01, -2.3883379e+01, ...,\n",
      "          1.2643364e+02,  7.5383514e+01, -9.4295639e+01],\n",
      "        [ 6.1439075e+01,  1.3702469e+02, -5.5815212e+01, ...,\n",
      "          1.4889339e+02,  1.8936537e+01, -2.1564900e+02],\n",
      "        [ 2.9349365e+01,  2.0578189e+02, -6.3144634e+01, ...,\n",
      "         -5.9076748e+01,  4.4466381e+01, -1.4979886e+02]]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "op = model(mix_inp__after_enc)\n",
    "pprint.pprint(op)   # shape 360 = sum of motion input len (120) and audio input len (240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./_expts/mix_op__after_enc.npy\", op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expt 2: Midpt *before* encoding - at the hash**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gWA_sFM_cAll_d27_mWA2_ch17'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'gWA_sFM_cAll_d27_mWA4_ch19'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name1, name2 = bytes.decode(inp1['motion_name'].numpy().item()), bytes.decode(inp2['motion_name'].numpy().item())\n",
    "display(name1, name2, type(name1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.09207153, 0.93508911, 0.37107849, 0.88606262, 0.72732544,\n",
       "         0.67869568, 0.9442749 , 0.04432678, 0.53126526, 0.56632996]]),\n",
       " array([[0.95883179, 0.61355591, 0.09431458, 0.64544678, 0.96304321,\n",
       "         0.27253723, 0.58055115, 0.97114563, 0.44119263, 0.12939453]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_hash_1, name_hash_2 = compute_hashed_name(name1), compute_hashed_name(name2)\n",
    "name_hash_1, name_hash_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52545166, 0.77432251, 0.23269653, 0.7657547 , 0.84518433,\n",
       "        0.47561646, 0.76241302, 0.50773621, 0.48622894, 0.34786224]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_hash = 0.5 * (name_hash_1 + name_hash_2)\n",
    "mix_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 219)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_inp = get_encoded_hash(mix_hash, enc_pkl_data)\n",
    "motion_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check with input hashes\n",
    "enc_hash_inp1 = get_encoded_hash(name_hash_1, enc_pkl_data)\n",
    "enc_hash_inp2 = get_encoded_hash(name_hash_2, enc_pkl_data)\n",
    "\n",
    "# inputs have padding & batch dim\n",
    "assert np.all(inp1['motion_input'].numpy()[0,:,:6] == 0)\n",
    "assert np.all(inp2['motion_input'].numpy()[0,:,:6] == 0)\n",
    "\n",
    "assert np.allclose(enc_hash_inp1[:120], inp1['motion_input'].numpy()[0,:,6:])   \n",
    "assert np.allclose(enc_hash_inp2[:120], inp2['motion_input'].numpy()[0,:,6:])\n",
    "\n",
    "assert not np.allclose(motion_inp, enc_hash_inp1)\n",
    "assert not np.allclose(motion_inp, enc_hash_inp2)\n",
    "assert not np.allclose(motion_inp, 0.5 * (enc_hash_inp1 + enc_hash_inp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 120, 225)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matching input dims\n",
    "motion_inp = np.pad(motion_inp, [[0,0], [6,0]])\n",
    "motion_inp = motion_inp[None, :120]\n",
    "motion_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_inp__before_enc_hash = copy.deepcopy(inp1)\n",
    "mix_inp__before_enc_hash['motion_input'] = motion_inp\n",
    "\n",
    "op = model(mix_inp__before_enc_hash)\n",
    "np.save(\"./_expts/mix_op__before_enc_hash.npy\", op)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73ef9dae1850d4826a6a2abacc613ceaa38b06453232522e74b0afa9475d9130"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('ai-choreo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
