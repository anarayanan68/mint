{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed mixed input - trial notebook\n",
    "\n",
    "1. Load model thru checkpoints, config & mint config utils\n",
    "    - Similar to `evaluator.py`\n",
    "1. Load tfrecord dataset as a starting point\n",
    "1. Mix up inputs in the dataset to create a new input (or new dataset, whichever is easier)\n",
    "1. Pass mixed input to model and see what it comes up with\n",
    "1. Prepend original input motion sequence and visualize prediction vs targets\n",
    "    - (targets being all the un-mixed inputs used to make the mixed input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: To use the exr data format, please install the OpenEXR package following the instructions detailed in the README at github.com/tensorflow/graphics.\n",
      "Warning: To use the threejs_vizualization, please install the colabtools package following the instructions detailed in the README at github.com/tensorflow/graphics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedWindow(verbose=True): could not load ipyvtklink try:\n",
      "> pip install ipyvtklink\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from mint.core import inputs\n",
    "from mint.core import model_builder\n",
    "from mint.ctl import single_task_evaluator\n",
    "from mint.utils import config_util\n",
    "from third_party.tf_models import orbit\n",
    "import tensorflow as tf\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import pprint\n",
    "import hashlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import copy\n",
    "import vedo\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from smplx import SMPL\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7669934fdf7443e8a214e3cede411cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Path to config file:'), Text(value='./configs/motion_enc_pilot-audiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# layout helpers\n",
    "\n",
    "layout_path_input = widgets.Layout(width='700px', height='40px')\n",
    "\n",
    "# input widgets\n",
    "\n",
    "wg_config_path = widgets.Text(\n",
    "    value=\"./configs/motion_enc_pilot-audioseed-37_bsz8.config\",\n",
    "    placeholder=\"Path to config file\",\n",
    "    layout=layout_path_input,\n",
    ")\n",
    "\n",
    "wg_checkpoint_dir = widgets.Text(\n",
    "    value=\"/srv/share4/anarayanan68/mint/_expts/motion_enc_pilot_bsz8_1GPU/checkpoints\",\n",
    "    placeholder=\"Checkpoint directory to restore model from\",\n",
    "    layout=layout_path_input,\n",
    ")\n",
    "\n",
    "wg_enc_pkl_path = widgets.Text(\n",
    "    value=\"/srv/share4/anarayanan68/mint/_expts/motion_enc_pilot_bsz8_1GPU/enc_data.pkl\",\n",
    "    placeholder=\"Path to pkl file for motion name encoding\",\n",
    "    layout=layout_path_input,\n",
    ")\n",
    "\n",
    "\n",
    "# overall container\n",
    "\n",
    "wg_container = widgets.VBox([\n",
    "    widgets.HBox([\n",
    "        widgets.Label(\"Path to config file:\"),\n",
    "        wg_config_path,\n",
    "    ]),\n",
    "    widgets.HBox([\n",
    "        widgets.Label(\"Checkpoint dir:\"),\n",
    "        wg_checkpoint_dir,\n",
    "    ]),\n",
    "    widgets.HBox([\n",
    "        widgets.Label(\"Path to encoding pkl file:\"),\n",
    "        wg_enc_pkl_path,\n",
    "    ]),\n",
    "])\n",
    "\n",
    "display(wg_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./configs/onehot_tvloss_overfit-audioseed37_bsz8.config',\n",
       " '/coc/scratch/anarayanan68/mint/_expts/onehot_tvloss_overfit_bsz8_1GPU/checkpoints',\n",
       " '/coc/scratch/anarayanan68/mint/_expts/onehot_tvloss_overfit_bsz8_1GPU/enc_data.pkl')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wg_config_path.value, wg_checkpoint_dir.value, wg_enc_pkl_path.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config read\n",
    "\n",
    "configs = config_util.get_configs_from_pipeline_file(wg_config_path.value)\n",
    "model_config = configs['model']\n",
    "eval_config = configs['eval_config']\n",
    "eval_dataset_config = configs['eval_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored model from /coc/scratch/anarayanan68/mint/_expts/onehot_tvloss_overfit_bsz8_1GPU/checkpoints/ckpt-29999.\n"
     ]
    }
   ],
   "source": [
    "# Model build & restore\n",
    "\n",
    "model = model_builder.build(model_config, is_training=False)   # even using True would work as the arg is unused\n",
    "\n",
    "checkpoint_manager=tf.train.CheckpointManager(\n",
    "    tf.train.Checkpoint(model=model),\n",
    "    directory=wg_checkpoint_dir.value,\n",
    "    max_to_keep=None)\n",
    "\n",
    "checkpoint_path = checkpoint_manager.restore_or_initialize()\n",
    "\n",
    "if checkpoint_path is not None:\n",
    "    print(f\"restored model from {checkpoint_path}.\")\n",
    "else:\n",
    "    print(\"initialized model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original dataset\n",
    "orig_dataset = inputs.create_input(\n",
    "      train_eval_config=eval_config,\n",
    "      dataset_config=eval_dataset_config,\n",
    "      is_training=False,\n",
    "      use_tpu=False,\n",
    "      overfit_expt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['audio_name', 'audio_sequence_shape', 'motion_name', 'motion_name_enc_shape', 'motion_sequence_shape', 'motion_input', 'target', 'actual_motion_input', 'audio_input']\n",
      "{'actual_motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[ 0.        ,  0.        ,  0.        , ..., -0.0012393 ,\n",
      "         -0.00159104,  0.999998  ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.00129578,\n",
      "         -0.00156013,  0.9999979 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.00141662,\n",
      "         -0.00139485,  0.99999803],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.00184672,\n",
      "         -0.00517991,  0.99998486],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.00180948,\n",
      "         -0.00516688,  0.99998504],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.00192659,\n",
      "         -0.0050804 ,  0.9999852 ]]], dtype=float32)>,\n",
      " 'audio_input': <tf.Tensor: shape=(1, 240, 35), dtype=float32, numpy=\n",
      "array([[[ -117.623375,  -254.53906 , -2164.5752  , ..., -1150.2084  ,\n",
      "         -2301.1704  , -1545.4581  ],\n",
      "        [-1313.4275  ,  -562.8319  ,    83.6605  , ..., -1475.6674  ,\n",
      "         -2226.9148  , -1312.3093  ],\n",
      "        [-1818.2975  , -1649.7328  , -1649.3789  , ...,  -909.8478  ,\n",
      "         -1147.3018  ,  -436.3925  ],\n",
      "        ...,\n",
      "        [-1393.6329  ,  -182.91045 ,   142.42725 , ..., -2240.0874  ,\n",
      "           556.301   , -1835.7988  ],\n",
      "        [   11.439382,   -95.222374, -2347.5752  , ...,  -238.74919 ,\n",
      "          -776.85925 ,   626.03186 ],\n",
      "        [ -420.3197  ,   323.3565  ,  -889.71027 , ...,  -800.77313 ,\n",
      "            96.45356 ,   323.16455 ]]], dtype=float32)>,\n",
      " 'audio_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA1'], dtype=object)>,\n",
      " 'audio_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[2721,   35]], dtype=int32)>,\n",
      " 'motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[  0.        ,   0.        ,   0.        , ...,  -4.8916326 ,\n",
      "         -10.640907  ,   2.0192614 ],\n",
      "        [  0.        ,   0.        ,   0.        , ...,   8.128893  ,\n",
      "          13.57251   ,   1.6014107 ],\n",
      "        [  0.        ,   0.        ,   0.        , ...,  -1.4745593 ,\n",
      "         -12.758852  ,   8.056378  ],\n",
      "        ...,\n",
      "        [  0.        ,   0.        ,   0.        , ...,  -1.8049462 ,\n",
      "          -8.229044  ,   0.42391253],\n",
      "        [  0.        ,   0.        ,   0.        , ...,   7.9458027 ,\n",
      "           5.7136936 ,  -5.976037  ],\n",
      "        [  0.        ,   0.        ,   0.        , ...,   3.9285371 ,\n",
      "          -4.422556  ,  -4.9068565 ]]], dtype=float32)>,\n",
      " 'motion_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d26_mWA1_ch09'], dtype=object)>,\n",
      " 'motion_name_enc_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[256, 219]], dtype=int32)>,\n",
      " 'motion_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[2558,  219]], dtype=int32)>,\n",
      " 'target': <tf.Tensor: shape=(1, 60, 225), dtype=float32, numpy=\n",
      "array([[[ 0.        ,  0.        ,  0.        , ..., -0.00206478,\n",
      "         -0.00500151,  0.99998534],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.00211833,\n",
      "         -0.00494229,  0.9999855 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.00209994,\n",
      "         -0.0048438 ,  0.99998605],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.0020692 ,\n",
      "         -0.00462998,  0.9999871 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.0021262 ,\n",
      "         -0.00475237,  0.99998647],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.0022668 ,\n",
      "         -0.00479513,  0.99998593]]], dtype=float32)>}\n",
      "1 ['audio_name', 'audio_sequence_shape', 'motion_name', 'motion_name_enc_shape', 'motion_sequence_shape', 'motion_input', 'target', 'actual_motion_input', 'audio_input']\n",
      "{'actual_motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -9.8180119e-04,  1.8328466e-03,  9.9999785e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -9.1753050e-04,  1.8183530e-03,  9.9999791e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -9.1931503e-04,  1.7792224e-03,  9.9999797e-01],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          4.3062118e-04,  2.0214997e-03,  9.9999785e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          3.8123975e-04,  1.8915540e-03,  9.9999815e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          3.3898835e-04,  1.8229444e-03,  9.9999827e-01]]], dtype=float32)>,\n",
      " 'audio_input': <tf.Tensor: shape=(1, 240, 35), dtype=float32, numpy=\n",
      "array([[[  -66.404236,  -187.17056 , -1871.9159  , ...,  -977.1947  ,\n",
      "         -1992.3997  , -1325.8243  ],\n",
      "        [-1121.162   ,  -459.0999  ,   111.13799 , ..., -1264.2655  ,\n",
      "         -1926.9025  , -1120.1757  ],\n",
      "        [-1566.482   , -1417.7998  , -1417.4875  , ...,  -765.18494 ,\n",
      "          -974.6309  ,  -347.5742  ],\n",
      "        ...,\n",
      "        [-1191.9071  ,  -123.990616,   162.97314 , ..., -1938.5214  ,\n",
      "           528.03    , -1581.9191  ],\n",
      "        [   47.435432,   -46.645454, -2033.3309  , ...,  -173.24313 ,\n",
      "          -647.8825  ,   589.5361  ],\n",
      "        [ -333.39722 ,   322.5616  ,  -747.42267 , ...,  -668.97577 ,\n",
      "           122.4221  ,   322.39227 ]]], dtype=float32)>,\n",
      " 'audio_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA2'], dtype=object)>,\n",
      " 'audio_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[2305,   35]], dtype=int32)>,\n",
      " 'motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[ 0.        ,  0.        ,  0.        , ...,  1.1826413 ,\n",
      "          6.879768  ,  2.5374863 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -8.773286  ,\n",
      "          5.8567758 ,  6.9013944 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -8.596235  ,\n",
      "          1.8503025 ,  0.46375874],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        , ...,  1.6149583 ,\n",
      "         -5.9365106 , 10.236451  ],\n",
      "        [ 0.        ,  0.        ,  0.        , ...,  4.2665606 ,\n",
      "          7.299572  , 14.17654   ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., 10.95846   ,\n",
      "          1.6824228 , -9.687096  ]]], dtype=float32)>,\n",
      " 'motion_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d27_mWA2_ch21'], dtype=object)>,\n",
      " 'motion_name_enc_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[256, 219]], dtype=int32)>,\n",
      " 'motion_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[2302,  219]], dtype=int32)>,\n",
      " 'target': <tf.Tensor: shape=(1, 60, 225), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          1.6754305e-04,  1.7935410e-03,  9.9999839e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          7.4286552e-05,  1.7554392e-03,  9.9999845e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -7.8935125e-05,  1.8950283e-03,  9.9999821e-01],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          4.2533418e-04, -1.5080088e-03,  9.9999875e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          4.3307920e-04, -1.4791678e-03,  9.9999881e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          4.4553741e-04, -1.3834765e-03,  9.9999893e-01]]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "# see samples in dataset\n",
    "for i,x in enumerate(orig_dataset):\n",
    "    if i >= 2:\n",
    "        break\n",
    "    print(i, list(x.keys()))\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each entry in the `orig_dataset` is the `inputs.py`-preprocessed dict of each `tfexample` written by `tools/preprocessing.py`**\n",
    "<br>\n",
    "So to make a new input, just create a dict that can be passed to the model just as `SingleTaskEvaluator` passes its `inputs`. No `tf` dataset necessary.\n",
    "<br>\n",
    "But, to compute the input, use some inputs from the `orig_dataset` and also the encoding scheme from `tools/preprocessing.py`. Also don't forget to use the actual motion inputs etc from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fns from preprocessing, for motion name encoding\n",
    "\n",
    "from tools.preprocessing import load_enc_pkl, get_latent_from_seq_name, encode_latent_vector, get_encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_pkl_data = load_enc_pkl(wg_enc_pkl_path.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'actual_motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[ 0.        ,  0.        ,  0.        , ..., -0.0012393 ,\n",
      "         -0.00159104,  0.999998  ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.00129578,\n",
      "         -0.00156013,  0.9999979 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.00141662,\n",
      "         -0.00139485,  0.99999803],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.00184672,\n",
      "         -0.00517991,  0.99998486],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.00180948,\n",
      "         -0.00516688,  0.99998504],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.00192659,\n",
      "         -0.0050804 ,  0.9999852 ]]], dtype=float32)>,\n",
      " 'audio_input': <tf.Tensor: shape=(1, 240, 35), dtype=float32, numpy=\n",
      "array([[[ -117.623375,  -254.53906 , -2164.5752  , ..., -1150.2084  ,\n",
      "         -2301.1704  , -1545.4581  ],\n",
      "        [-1313.4275  ,  -562.8319  ,    83.6605  , ..., -1475.6674  ,\n",
      "         -2226.9148  , -1312.3093  ],\n",
      "        [-1818.2975  , -1649.7328  , -1649.3789  , ...,  -909.8478  ,\n",
      "         -1147.3018  ,  -436.3925  ],\n",
      "        ...,\n",
      "        [-1393.6329  ,  -182.91045 ,   142.42725 , ..., -2240.0874  ,\n",
      "           556.301   , -1835.7988  ],\n",
      "        [   11.439382,   -95.222374, -2347.5752  , ...,  -238.74919 ,\n",
      "          -776.85925 ,   626.03186 ],\n",
      "        [ -420.3197  ,   323.3565  ,  -889.71027 , ...,  -800.77313 ,\n",
      "            96.45356 ,   323.16455 ]]], dtype=float32)>,\n",
      " 'audio_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA1'], dtype=object)>,\n",
      " 'audio_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[2721,   35]], dtype=int32)>,\n",
      " 'motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[  0.        ,   0.        ,   0.        , ...,  -4.8916326 ,\n",
      "         -10.640907  ,   2.0192614 ],\n",
      "        [  0.        ,   0.        ,   0.        , ...,   8.128893  ,\n",
      "          13.57251   ,   1.6014107 ],\n",
      "        [  0.        ,   0.        ,   0.        , ...,  -1.4745593 ,\n",
      "         -12.758852  ,   8.056378  ],\n",
      "        ...,\n",
      "        [  0.        ,   0.        ,   0.        , ...,  -1.8049462 ,\n",
      "          -8.229044  ,   0.42391253],\n",
      "        [  0.        ,   0.        ,   0.        , ...,   7.9458027 ,\n",
      "           5.7136936 ,  -5.976037  ],\n",
      "        [  0.        ,   0.        ,   0.        , ...,   3.9285371 ,\n",
      "          -4.422556  ,  -4.9068565 ]]], dtype=float32)>,\n",
      " 'motion_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d26_mWA1_ch09'], dtype=object)>,\n",
      " 'motion_name_enc_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[256, 219]], dtype=int32)>,\n",
      " 'motion_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[2558,  219]], dtype=int32)>,\n",
      " 'target': <tf.Tensor: shape=(1, 60, 225), dtype=float32, numpy=\n",
      "array([[[ 0.        ,  0.        ,  0.        , ..., -0.00206478,\n",
      "         -0.00500151,  0.99998534],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.00211833,\n",
      "         -0.00494229,  0.9999855 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.00209994,\n",
      "         -0.0048438 ,  0.99998605],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.0020692 ,\n",
      "         -0.00462998,  0.9999871 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.0021262 ,\n",
      "         -0.00475237,  0.99998647],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.0022668 ,\n",
      "         -0.00479513,  0.99998593]]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "it = iter(orig_dataset)\n",
    "inp1 = next(it)\n",
    "inp2 = next(it)\n",
    "\n",
    "pprint.pprint(inp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'actual_motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -9.8180119e-04,  1.8328466e-03,  9.9999785e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -9.1753050e-04,  1.8183530e-03,  9.9999791e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -9.1931503e-04,  1.7792224e-03,  9.9999797e-01],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          4.3062118e-04,  2.0214997e-03,  9.9999785e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          3.8123975e-04,  1.8915540e-03,  9.9999815e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          3.3898835e-04,  1.8229444e-03,  9.9999827e-01]]], dtype=float32)>,\n",
      " 'audio_input': <tf.Tensor: shape=(1, 240, 35), dtype=float32, numpy=\n",
      "array([[[  -66.404236,  -187.17056 , -1871.9159  , ...,  -977.1947  ,\n",
      "         -1992.3997  , -1325.8243  ],\n",
      "        [-1121.162   ,  -459.0999  ,   111.13799 , ..., -1264.2655  ,\n",
      "         -1926.9025  , -1120.1757  ],\n",
      "        [-1566.482   , -1417.7998  , -1417.4875  , ...,  -765.18494 ,\n",
      "          -974.6309  ,  -347.5742  ],\n",
      "        ...,\n",
      "        [-1191.9071  ,  -123.990616,   162.97314 , ..., -1938.5214  ,\n",
      "           528.03    , -1581.9191  ],\n",
      "        [   47.435432,   -46.645454, -2033.3309  , ...,  -173.24313 ,\n",
      "          -647.8825  ,   589.5361  ],\n",
      "        [ -333.39722 ,   322.5616  ,  -747.42267 , ...,  -668.97577 ,\n",
      "           122.4221  ,   322.39227 ]]], dtype=float32)>,\n",
      " 'audio_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA2'], dtype=object)>,\n",
      " 'audio_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[2305,   35]], dtype=int32)>,\n",
      " 'motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[ 0.        ,  0.        ,  0.        , ...,  1.1826413 ,\n",
      "          6.879768  ,  2.5374863 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -8.773286  ,\n",
      "          5.8567758 ,  6.9013944 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -8.596235  ,\n",
      "          1.8503025 ,  0.46375874],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        , ...,  1.6149583 ,\n",
      "         -5.9365106 , 10.236451  ],\n",
      "        [ 0.        ,  0.        ,  0.        , ...,  4.2665606 ,\n",
      "          7.299572  , 14.17654   ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., 10.95846   ,\n",
      "          1.6824228 , -9.687096  ]]], dtype=float32)>,\n",
      " 'motion_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d27_mWA2_ch21'], dtype=object)>,\n",
      " 'motion_name_enc_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[256, 219]], dtype=int32)>,\n",
      " 'motion_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[2302,  219]], dtype=int32)>,\n",
      " 'target': <tf.Tensor: shape=(1, 60, 225), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          1.6754305e-04,  1.7935410e-03,  9.9999839e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          7.4286552e-05,  1.7554392e-03,  9.9999845e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -7.8935125e-05,  1.8950283e-03,  9.9999821e-01],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          4.2533418e-04, -1.5080088e-03,  9.9999875e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          4.3307920e-04, -1.4791678e-03,  9.9999881e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          4.4553741e-04, -1.3834765e-03,  9.9999893e-01]]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(inp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: passing actual input and saving output\n",
    "op_inp1 = model(inp1)\n",
    "np.save(\"./_expts/op_inp1.npy\", op_inp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_inp2 = model(inp2)\n",
    "np.save(\"./_expts/op_inp2.npy\", op_inp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 60, 225])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA1'], dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d26_mWA1_ch09'], dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(inp1['target'].shape)\n",
    "display(inp1['audio_name'], inp1['motion_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 60, 225])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA2'], dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d27_mWA2_ch21'], dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(inp2['target'].shape)\n",
    "display(inp2['audio_name'], inp2['motion_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expt 1: Midpt *after* encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
       "array([[[ 0.        ,  0.        ,  0.        , ..., -1.8544956 ,\n",
       "         -1.8805697 ,  2.2783737 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.32219648,\n",
       "          9.714643  ,  4.2514024 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -5.0353975 ,\n",
       "         -5.4542747 ,  4.2600684 ],\n",
       "        ...,\n",
       "        [ 0.        ,  0.        ,  0.        , ..., -0.09499395,\n",
       "         -7.082777  ,  5.330182  ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  6.1061816 ,\n",
       "          6.506633  ,  4.1002517 ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  7.4434986 ,\n",
       "         -1.3700666 , -7.296976  ]]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_inp = 0.5 * (inp1['motion_input'] + inp2['motion_input'])\n",
    "motion_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['audio_name',\n",
       " 'audio_sequence_shape',\n",
       " 'motion_name',\n",
       " 'motion_name_enc_shape',\n",
       " 'motion_sequence_shape',\n",
       " 'motion_input',\n",
       " 'target',\n",
       " 'actual_motion_input',\n",
       " 'audio_input']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(inp1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'actual_motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[ 0.        ,  0.        ,  0.        , ..., -0.0012393 ,\n",
      "         -0.00159104,  0.999998  ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.00129578,\n",
      "         -0.00156013,  0.9999979 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.00141662,\n",
      "         -0.00139485,  0.99999803],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.00184672,\n",
      "         -0.00517991,  0.99998486],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.00180948,\n",
      "         -0.00516688,  0.99998504],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.00192659,\n",
      "         -0.0050804 ,  0.9999852 ]]], dtype=float32)>,\n",
      " 'audio_input': <tf.Tensor: shape=(1, 240, 35), dtype=float32, numpy=\n",
      "array([[[ -117.623375,  -254.53906 , -2164.5752  , ..., -1150.2084  ,\n",
      "         -2301.1704  , -1545.4581  ],\n",
      "        [-1313.4275  ,  -562.8319  ,    83.6605  , ..., -1475.6674  ,\n",
      "         -2226.9148  , -1312.3093  ],\n",
      "        [-1818.2975  , -1649.7328  , -1649.3789  , ...,  -909.8478  ,\n",
      "         -1147.3018  ,  -436.3925  ],\n",
      "        ...,\n",
      "        [-1393.6329  ,  -182.91045 ,   142.42725 , ..., -2240.0874  ,\n",
      "           556.301   , -1835.7988  ],\n",
      "        [   11.439382,   -95.222374, -2347.5752  , ...,  -238.74919 ,\n",
      "          -776.85925 ,   626.03186 ],\n",
      "        [ -420.3197  ,   323.3565  ,  -889.71027 , ...,  -800.77313 ,\n",
      "            96.45356 ,   323.16455 ]]], dtype=float32)>,\n",
      " 'audio_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA1'], dtype=object)>,\n",
      " 'audio_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[2721,   35]], dtype=int32)>,\n",
      " 'motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[ 0.        ,  0.        ,  0.        , ..., -1.8544956 ,\n",
      "         -1.8805697 ,  2.2783737 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.32219648,\n",
      "          9.714643  ,  4.2514024 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -5.0353975 ,\n",
      "         -5.4542747 ,  4.2600684 ],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.09499395,\n",
      "         -7.082777  ,  5.330182  ],\n",
      "        [ 0.        ,  0.        ,  0.        , ...,  6.1061816 ,\n",
      "          6.506633  ,  4.1002517 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ...,  7.4434986 ,\n",
      "         -1.3700666 , -7.296976  ]]], dtype=float32)>,\n",
      " 'motion_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d26_mWA1_ch09'], dtype=object)>,\n",
      " 'motion_name_enc_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[256, 219]], dtype=int32)>,\n",
      " 'motion_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[2558,  219]], dtype=int32)>,\n",
      " 'target': <tf.Tensor: shape=(1, 60, 225), dtype=float32, numpy=\n",
      "array([[[ 0.        ,  0.        ,  0.        , ..., -0.00206478,\n",
      "         -0.00500151,  0.99998534],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.00211833,\n",
      "         -0.00494229,  0.9999855 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.00209994,\n",
      "         -0.0048438 ,  0.99998605],\n",
      "        ...,\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.0020692 ,\n",
      "         -0.00462998,  0.9999871 ],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.0021262 ,\n",
      "         -0.00475237,  0.99998647],\n",
      "        [ 0.        ,  0.        ,  0.        , ..., -0.0022668 ,\n",
      "         -0.00479513,  0.99998593]]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "mix_inp__after_enc = copy.deepcopy(inp1)\n",
    "mix_inp__after_enc['motion_input'] = motion_inp\n",
    "\n",
    "pprint.pprint(mix_inp__after_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Tensor: shape=(1, 360, 225), dtype=float32, numpy=\n",
      "array([[[ 2.19420135e-01,  1.05319418e-01,  1.24800466e-02, ...,\n",
      "          5.99160604e-02, -1.57481223e-01,  1.07862961e+00],\n",
      "        [ 5.71451709e-02,  2.84978151e-01,  1.22801930e-01, ...,\n",
      "          3.24298620e-01, -2.31692374e-01,  8.86855125e-01],\n",
      "        [ 1.29207015e-01, -8.50098357e-02, -7.02601969e-02, ...,\n",
      "          1.12366292e-03, -3.22265804e-01,  1.01654541e+00],\n",
      "        ...,\n",
      "        [-2.17097916e+02, -1.61895615e+02,  1.05281868e+02, ...,\n",
      "          1.71287556e+01, -1.68510651e+02, -3.18635040e+02],\n",
      "        [ 5.00909004e+01,  6.67435608e+01, -4.03104591e+01, ...,\n",
      "         -7.39148102e+01, -8.78099060e+01, -1.23307236e+02],\n",
      "        [ 1.35903229e+02,  1.58804642e+02,  1.04776405e+02, ...,\n",
      "         -1.36388611e+02,  2.91796613e+00, -1.19153114e+02]]],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "op = model(mix_inp__after_enc)\n",
    "pprint.pprint(op)   # shape 360 = sum of motion input len (120) and audio input len (240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./_expts/mix_op__after_enc.npy\", op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expt 2: Midpt *before* encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gWA_sFM_cAll_d26_mWA1_ch09'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'gWA_sFM_cAll_d27_mWA2_ch21'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name1, name2 = bytes.decode(inp1['motion_name'].numpy().item()), bytes.decode(inp2['motion_name'].numpy().item())\n",
    "display(name1, name2, type(name1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.]),\n",
       " array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_latent_1, name_latent_2 = get_latent_from_seq_name(name1, enc_pkl_data), get_latent_from_seq_name(name2, enc_pkl_data)\n",
    "name_latent_1, name_latent_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0. , 0.5, 0. , 0. , 0. , 0. , 0. , 0. , 0.5, 0. , 0. , 0. ,\n",
       "       0. , 0. , 0. , 0. , 0. , 0. , 0. ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_latent = 0.5 * (name_latent_1 + name_latent_2)\n",
    "mix_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 219)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_inp = encode_latent_vector(mix_latent, enc_pkl_data)\n",
    "motion_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check with input hashes\n",
    "enc_latent_inp1 = encode_latent_vector(name_latent_1, enc_pkl_data)\n",
    "enc_latent_inp2 = encode_latent_vector(name_latent_2, enc_pkl_data)\n",
    "\n",
    "# inputs have padding & batch dim\n",
    "assert np.all(inp1['motion_input'].numpy()[0,:,:6] == 0)\n",
    "assert np.all(inp2['motion_input'].numpy()[0,:,:6] == 0)\n",
    "\n",
    "assert np.allclose(enc_latent_inp1[:120], inp1['motion_input'].numpy()[0,:,6:])   \n",
    "assert np.allclose(enc_latent_inp2[:120], inp2['motion_input'].numpy()[0,:,6:])\n",
    "\n",
    "assert not np.allclose(motion_inp, enc_latent_inp1)\n",
    "assert not np.allclose(motion_inp, enc_latent_inp2)\n",
    "assert not np.allclose(motion_inp, 0.5 * (enc_latent_inp1 + enc_latent_inp2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 120, 225)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matching input dims\n",
    "motion_inp = np.pad(motion_inp, [[0,0], [6,0]])\n",
    "motion_inp = motion_inp[None, :120]\n",
    "motion_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_inp__before_enc = copy.deepcopy(inp1)\n",
    "mix_inp__before_enc['motion_input'] = motion_inp\n",
    "\n",
    "op = model(mix_inp__before_enc)\n",
    "np.save(\"./_expts/mix_op__before_enc.npy\", op)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73ef9dae1850d4826a6a2abacc613ceaa38b06453232522e74b0afa9475d9130"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('ai-choreo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
