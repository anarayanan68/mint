{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed mixed input - trial notebook\n",
    "\n",
    "1. Load model thru checkpoints, config & mint config utils\n",
    "    - Similar to `evaluator.py`\n",
    "1. Load tfrecord dataset as a starting point\n",
    "1. Mix up inputs in the dataset to create a new input (or new dataset, whichever is easier)\n",
    "1. Pass mixed input to model and see what it comes up with\n",
    "1. Prepend original input motion sequence and visualize prediction vs targets\n",
    "    - (targets being all the un-mixed inputs used to make the mixed input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: To use the exr data format, please install the OpenEXR package following the instructions detailed in the README at github.com/tensorflow/graphics.\n",
      "Warning: To use the threejs_vizualization, please install the colabtools package following the instructions detailed in the README at github.com/tensorflow/graphics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedWindow(verbose=True): could not load ipyvtklink try:\n",
      "> pip install ipyvtklink\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from mint.core import inputs\n",
    "from mint.core import model_builder\n",
    "from mint.ctl import single_task_evaluator\n",
    "from mint.utils import config_util\n",
    "from third_party.tf_models import orbit\n",
    "import tensorflow as tf\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import pprint\n",
    "import hashlib\n",
    "import pickle\n",
    "import numpy as np\n",
    "import copy\n",
    "import vedo\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from smplx import SMPL\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46cc224c00fe4597aed3e5fa5f14a037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='Path to config file:'), Text(value='./configs/motion_enc_pilot-audiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# layout helpers\n",
    "\n",
    "layout_path_input = widgets.Layout(width='700px', height='40px')\n",
    "\n",
    "# input widgets\n",
    "\n",
    "wg_config_path = widgets.Text(\n",
    "    value=\"./configs/motion_enc_pilot-audioseed-37_bsz8.config\",\n",
    "    placeholder=\"Path to config file\",\n",
    "    layout=layout_path_input,\n",
    ")\n",
    "\n",
    "wg_checkpoint_dir = widgets.Text(\n",
    "    value=\"/srv/share4/anarayanan68/mint/_expts/motion_enc_pilot_bsz8_1GPU/checkpoints\",\n",
    "    placeholder=\"Checkpoint directory to restore model from\",\n",
    "    layout=layout_path_input,\n",
    ")\n",
    "\n",
    "wg_enc_pkl_path = widgets.Text(\n",
    "    value=\"/srv/share4/anarayanan68/mint/_expts/motion_enc_pilot_bsz8_1GPU/enc_data.pkl\",\n",
    "    placeholder=\"Path to pkl file for motion name encoding\",\n",
    "    layout=layout_path_input,\n",
    ")\n",
    "\n",
    "wg_name_enc_yaml_path = widgets.Text(\n",
    "    value=\"./configs/joint_optim_pilot-name_enc.yml\",\n",
    "    placeholder=\"Path to name encoder YAML config file\",\n",
    "    layout=layout_path_input,\n",
    ")\n",
    "\n",
    "# overall container\n",
    "\n",
    "wg_container = widgets.VBox([\n",
    "    widgets.HBox([\n",
    "        widgets.Label(\"Path to config file:\"),\n",
    "        wg_config_path,\n",
    "    ]),\n",
    "    widgets.HBox([\n",
    "        widgets.Label(\"Checkpoint dir:\"),\n",
    "        wg_checkpoint_dir,\n",
    "    ]),\n",
    "    widgets.HBox([\n",
    "        widgets.Label(\"Path to encoding pkl file:\"),\n",
    "        wg_enc_pkl_path,\n",
    "    ]),\n",
    "    widgets.HBox([\n",
    "        widgets.Label(\"Path to name enc YAML config file:\"),\n",
    "        wg_name_enc_yaml_path,\n",
    "    ]),\n",
    "])\n",
    "\n",
    "display(wg_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./configs/joint_optim_pilot-audioseed37_bsz8.config',\n",
       " './_expts/joint_optim_pilot_bsz8_1GPU/checkpoints',\n",
       " './_expts/joint_optim_pilot_bsz8_1GPU/enc_data.pkl',\n",
       " './configs/joint_optim_pilot-name_enc.yml')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wg_config_path.value, wg_checkpoint_dir.value, wg_enc_pkl_path.value, wg_name_enc_yaml_path.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config read\n",
    "\n",
    "configs = config_util.get_configs_from_pipeline_file(wg_config_path.value)\n",
    "model_config = configs['model']\n",
    "eval_config = configs['eval_config']\n",
    "eval_dataset_config = configs['eval_dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_enc_config_yaml = config_util.read_yaml_config(wg_name_enc_yaml_path.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restored model from ./_expts/joint_optim_pilot_bsz8_1GPU/checkpoints/ckpt-27001.\n"
     ]
    }
   ],
   "source": [
    "# Model build & restore\n",
    "\n",
    "model = model_builder.build(model_config, is_training=False,\n",
    "    name_encoder_config_yaml=name_enc_config_yaml, dataset_config=eval_dataset_config)\n",
    "\n",
    "checkpoint_manager=tf.train.CheckpointManager(\n",
    "    tf.train.Checkpoint(model=model),\n",
    "    directory=wg_checkpoint_dir.value,\n",
    "    max_to_keep=None)\n",
    "\n",
    "checkpoint_path = checkpoint_manager.restore_or_initialize()\n",
    "\n",
    "if checkpoint_path is not None:\n",
    "    print(f\"restored model from {checkpoint_path}.\")\n",
    "else:\n",
    "    print(\"initialized model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original dataset\n",
    "orig_dataset = inputs.create_input(\n",
    "      train_eval_config=eval_config,\n",
    "      dataset_config=eval_dataset_config,\n",
    "      is_training=False,\n",
    "      use_tpu=False,\n",
    "      overfit_expt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['motion_name_enc', 'audio_name', 'audio_sequence_shape', 'motion_name', 'motion_sequence_shape', 'target', 'actual_motion_input', 'audio_input']\n",
      "{'actual_motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -1.5071713e-03, -1.2368782e-03,  9.9999809e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -1.6000727e-03, -1.2885372e-03,  9.9999791e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -2.0099527e-03, -1.2556252e-03,  9.9999720e-01],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -4.9028394e-04, -5.5003497e-03,  9.9998474e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -4.2695715e-04, -5.1946845e-03,  9.9998641e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          7.4367061e-05, -4.8195366e-03,  9.9998838e-01]]], dtype=float32)>,\n",
      " 'audio_input': <tf.Tensor: shape=(1, 240, 35), dtype=float32, numpy=\n",
      "array([[[ -114.73737 ,  -246.10489 , -2078.7415  , ..., -1105.4794  ,\n",
      "         -2209.8015  , -1484.7126  ],\n",
      "        [-1262.0845  ,  -541.9049  ,    78.38997 , ..., -1417.75    ,\n",
      "         -2138.555   , -1261.0116  ],\n",
      "        [-1746.4958  , -1584.7618  , -1584.4222  , ...,  -874.8588  ,\n",
      "         -1102.6906  ,  -420.58914 ],\n",
      "        ...,\n",
      "        [-1339.0398  ,  -177.37885 ,   134.77534 , ..., -2151.1938  ,\n",
      "           531.87787 , -1763.288   ],\n",
      "        [    9.095432,   -93.24412 , -2254.326   , ...,  -230.95486 ,\n",
      "          -747.25934 ,   598.7831  ],\n",
      "        [ -405.16766 ,   308.3729  ,  -855.53735 , ...,  -770.20416 ,\n",
      "            90.66462 ,   308.1887  ]]], dtype=float32)>,\n",
      " 'audio_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mLH2'], dtype=object)>,\n",
      " 'audio_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[2449,   35]], dtype=int32)>,\n",
      " 'motion_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gLH_sBM_cAll_d18_mLH2_ch06'], dtype=object)>,\n",
      " 'motion_name_enc': <tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0.]], dtype=float32)>,\n",
      " 'motion_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[576, 219]], dtype=int32)>,\n",
      " 'target': <tf.Tensor: shape=(1, 60, 225), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          2.8984877e-04, -4.6250462e-03,  9.9998927e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          1.8257955e-04, -4.7070445e-03,  9.9998891e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          2.2820606e-04, -4.6233055e-03,  9.9998927e-01],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          1.4306977e-04, -5.2615181e-03,  9.9998617e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          4.0679163e-04, -5.2196495e-03,  9.9998629e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          6.2248821e-04, -5.1915417e-03,  9.9998635e-01]]], dtype=float32)>}\n",
      "1 ['motion_name_enc', 'audio_name', 'audio_sequence_shape', 'motion_name', 'motion_sequence_shape', 'target', 'actual_motion_input', 'audio_input']\n",
      "{'actual_motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          7.3011895e-04, -3.3532334e-03,  9.9999410e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          7.8092772e-04, -3.3680149e-03,  9.9999404e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          9.4373431e-04, -3.4757780e-03,  9.9999350e-01],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          8.2552433e-04, -3.0448267e-03,  9.9999505e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          6.3653680e-04, -2.8562604e-03,  9.9999571e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          2.7560751e-04, -2.6339253e-03,  9.9999648e-01]]], dtype=float32)>,\n",
      " 'audio_input': <tf.Tensor: shape=(1, 240, 35), dtype=float32, numpy=\n",
      "array([[[ 228.6918   ,  177.93639  , -530.1248   , ..., -154.09326  ,\n",
      "         -580.7614   , -300.6146   ],\n",
      "        [-214.59952  ,   63.650505 ,  303.30887  , ..., -274.74274  ,\n",
      "         -553.23444  , -214.185    ],\n",
      "        [-401.7577   , -339.2698   , -339.13858  , ...,  -64.99025  ,\n",
      "         -153.01575  ,  110.52231  ],\n",
      "        ...,\n",
      "        [-244.33214  ,  204.48953  ,  325.09406  , ..., -558.11755  ,\n",
      "          478.51938  , -408.24554  ],\n",
      "        [ 276.5361   ,  236.99599  , -597.9639   , ...,  183.78978  ,\n",
      "          -15.6906395,  504.36902  ],\n",
      "        [ 116.480576 ,  392.16553  ,  -57.525166 , ...,  -24.555668 ,\n",
      "          308.05133  ,  392.0944   ]]], dtype=float32)>,\n",
      " 'audio_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA5'], dtype=object)>,\n",
      " 'audio_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1773,   35]], dtype=int32)>,\n",
      " 'motion_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d25_mWA5_ch07'], dtype=object)>,\n",
      " 'motion_name_enc': <tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.]], dtype=float32)>,\n",
      " 'motion_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1771,  219]], dtype=int32)>,\n",
      " 'target': <tf.Tensor: shape=(1, 60, 225), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -6.4011406e-06, -2.3110723e-03,  9.9999732e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -1.7321896e-04, -2.0113811e-03,  9.9999797e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -4.5118894e-04, -1.7427352e-03,  9.9999839e-01],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          5.3984323e-04,  2.1051077e-04,  9.9999982e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          6.2756753e-04,  2.3758769e-04,  9.9999976e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          9.3505002e-04,  1.7698885e-04,  9.9999952e-01]]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "# see samples in dataset\n",
    "for i,x in enumerate(orig_dataset):\n",
    "    if i >= 2:\n",
    "        break\n",
    "    print(i, list(x.keys()))\n",
    "    pprint.pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each entry in the `orig_dataset` is the `inputs.py`-preprocessed dict of each `tfexample` written by `tools/preprocessing.py`**\n",
    "<br>\n",
    "So to make a new input, just create a dict that can be passed to the model just as `SingleTaskEvaluator` passes its `inputs`. No `tf` dataset necessary.\n",
    "<br>\n",
    "But, to compute the input, use some inputs from the `orig_dataset` and also the encoding scheme from `tools/preprocessing.py`. Also don't forget to use the actual motion inputs etc from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fns from preprocessing, for motion name encoding\n",
    "\n",
    "from tools import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'onehot_enc_map': {'len': 20,\n",
       "  'gWA_sFM_cAll_d25_mWA4_ch05': {'index': 0,\n",
       "   'enc': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d25_mWA2_ch03': {'index': 1,\n",
       "   'enc': array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d27_mWA2_ch21': {'index': 2,\n",
       "   'enc': array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d25_mWA5_ch07': {'index': 3,\n",
       "   'enc': array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d27_mWA2_ch17': {'index': 4,\n",
       "   'enc': array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d25_mWA5_ch06': {'index': 5,\n",
       "   'enc': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d26_mWA5_ch13': {'index': 6,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d25_mWA1_ch02': {'index': 7,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d27_mWA3_ch18': {'index': 8,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d26_mWA1_ch09': {'index': 9,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d27_mWA4_ch19': {'index': 10,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d26_mWA3_ch14': {'index': 11,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d27_mWA5_ch20': {'index': 12,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d25_mWA3_ch04': {'index': 13,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d26_mWA4_ch12': {'index': 14,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gWA_sFM_cAll_d26_mWA3_ch11': {'index': 15,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "          0., 0., 0.])},\n",
       "  'gLH_sBM_cAll_d18_mLH2_ch06': {'index': 16,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 0., 0.])},\n",
       "  'gLH_sBM_cAll_d17_mLH0_ch09': {'index': 17,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          1., 0., 0.])},\n",
       "  'gLH_sBM_cAll_d17_mLH1_ch09': {'index': 18,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 1., 0.])},\n",
       "  'gLH_sBM_cAll_d16_mLH2_ch04': {'index': 19,\n",
       "   'enc': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 1.])}}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_pkl_data = preprocessing.load_enc_pkl(wg_enc_pkl_path.value)\n",
    "enc_pkl_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'actual_motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -1.5071713e-03, -1.2368782e-03,  9.9999809e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -1.6000727e-03, -1.2885372e-03,  9.9999791e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -2.0099527e-03, -1.2556252e-03,  9.9999720e-01],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -4.9028394e-04, -5.5003497e-03,  9.9998474e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -4.2695715e-04, -5.1946845e-03,  9.9998641e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          7.4367061e-05, -4.8195366e-03,  9.9998838e-01]]], dtype=float32)>,\n",
      " 'audio_input': <tf.Tensor: shape=(1, 240, 35), dtype=float32, numpy=\n",
      "array([[[ -114.73737 ,  -246.10489 , -2078.7415  , ..., -1105.4794  ,\n",
      "         -2209.8015  , -1484.7126  ],\n",
      "        [-1262.0845  ,  -541.9049  ,    78.38997 , ..., -1417.75    ,\n",
      "         -2138.555   , -1261.0116  ],\n",
      "        [-1746.4958  , -1584.7618  , -1584.4222  , ...,  -874.8588  ,\n",
      "         -1102.6906  ,  -420.58914 ],\n",
      "        ...,\n",
      "        [-1339.0398  ,  -177.37885 ,   134.77534 , ..., -2151.1938  ,\n",
      "           531.87787 , -1763.288   ],\n",
      "        [    9.095432,   -93.24412 , -2254.326   , ...,  -230.95486 ,\n",
      "          -747.25934 ,   598.7831  ],\n",
      "        [ -405.16766 ,   308.3729  ,  -855.53735 , ...,  -770.20416 ,\n",
      "            90.66462 ,   308.1887  ]]], dtype=float32)>,\n",
      " 'audio_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mLH2'], dtype=object)>,\n",
      " 'audio_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[2449,   35]], dtype=int32)>,\n",
      " 'motion_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gLH_sBM_cAll_d18_mLH2_ch06'], dtype=object)>,\n",
      " 'motion_name_enc': <tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0.]], dtype=float32)>,\n",
      " 'motion_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[576, 219]], dtype=int32)>,\n",
      " 'target': <tf.Tensor: shape=(1, 60, 225), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          2.8984877e-04, -4.6250462e-03,  9.9998927e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          1.8257955e-04, -4.7070445e-03,  9.9998891e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          2.2820606e-04, -4.6233055e-03,  9.9998927e-01],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          1.4306977e-04, -5.2615181e-03,  9.9998617e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          4.0679163e-04, -5.2196495e-03,  9.9998629e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          6.2248821e-04, -5.1915417e-03,  9.9998635e-01]]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "it = iter(orig_dataset)\n",
    "inp1 = next(it)\n",
    "inp2 = next(it)\n",
    "\n",
    "pprint.pprint(inp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'actual_motion_input': <tf.Tensor: shape=(1, 120, 225), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          7.3011895e-04, -3.3532334e-03,  9.9999410e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          7.8092772e-04, -3.3680149e-03,  9.9999404e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          9.4373431e-04, -3.4757780e-03,  9.9999350e-01],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          8.2552433e-04, -3.0448267e-03,  9.9999505e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          6.3653680e-04, -2.8562604e-03,  9.9999571e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          2.7560751e-04, -2.6339253e-03,  9.9999648e-01]]], dtype=float32)>,\n",
      " 'audio_input': <tf.Tensor: shape=(1, 240, 35), dtype=float32, numpy=\n",
      "array([[[ 228.6918   ,  177.93639  , -530.1248   , ..., -154.09326  ,\n",
      "         -580.7614   , -300.6146   ],\n",
      "        [-214.59952  ,   63.650505 ,  303.30887  , ..., -274.74274  ,\n",
      "         -553.23444  , -214.185    ],\n",
      "        [-401.7577   , -339.2698   , -339.13858  , ...,  -64.99025  ,\n",
      "         -153.01575  ,  110.52231  ],\n",
      "        ...,\n",
      "        [-244.33214  ,  204.48953  ,  325.09406  , ..., -558.11755  ,\n",
      "          478.51938  , -408.24554  ],\n",
      "        [ 276.5361   ,  236.99599  , -597.9639   , ...,  183.78978  ,\n",
      "          -15.6906395,  504.36902  ],\n",
      "        [ 116.480576 ,  392.16553  ,  -57.525166 , ...,  -24.555668 ,\n",
      "          308.05133  ,  392.0944   ]]], dtype=float32)>,\n",
      " 'audio_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA5'], dtype=object)>,\n",
      " 'audio_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1773,   35]], dtype=int32)>,\n",
      " 'motion_name': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d25_mWA5_ch07'], dtype=object)>,\n",
      " 'motion_name_enc': <tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.]], dtype=float32)>,\n",
      " 'motion_sequence_shape': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[1771,  219]], dtype=int32)>,\n",
      " 'target': <tf.Tensor: shape=(1, 60, 225), dtype=float32, numpy=\n",
      "array([[[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -6.4011406e-06, -2.3110723e-03,  9.9999732e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -1.7321896e-04, -2.0113811e-03,  9.9999797e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "         -4.5118894e-04, -1.7427352e-03,  9.9999839e-01],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          5.3984323e-04,  2.1051077e-04,  9.9999982e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          6.2756753e-04,  2.3758769e-04,  9.9999976e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          9.3505002e-04,  1.7698885e-04,  9.9999952e-01]]], dtype=float32)>}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(inp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: passing actual input and saving output\n",
    "op_inp1 = model(inp1)\n",
    "np.save(\"./_expts/op_inp1.npy\", op_inp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_inp2 = model(inp2)\n",
    "np.save(\"./_expts/op_inp2.npy\", op_inp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 60, 225])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mLH2'], dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gLH_sBM_cAll_d18_mLH2_ch06'], dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(inp1['target'].shape)\n",
    "display(inp1['audio_name'], inp1['motion_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 60, 225])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'mWA5'], dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'gWA_sFM_cAll_d25_mWA5_ch07'], dtype=object)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(inp2['target'].shape)\n",
    "display(inp2['audio_name'], inp2['motion_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expt 1: Midpt *after* encoding** - NOT POSSIBLE (would need to modify the model to do this now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expt 2: Midpt *before* encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gLH_sBM_cAll_d18_mLH2_ch06'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'gWA_sFM_cAll_d25_mWA5_ch07'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name1, name2 = bytes.decode(inp1['motion_name'].numpy().item()), bytes.decode(inp2['motion_name'].numpy().item())\n",
    "display(name1, name2, type(name1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0.]),\n",
       " array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_latent_1, name_latent_2 = (\n",
    "    preprocessing.get_latent_from_seq_name(name1, enc_pkl_data),\n",
    "    preprocessing.get_latent_from_seq_name(name2, enc_pkl_data)\n",
    ")\n",
    "name_latent_1, name_latent_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0. , 0. , 0.5, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ,\n",
       "       0. , 0. , 0. , 0.5, 0. , 0. , 0. ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix_latent = 0.5 * (name_latent_1 + name_latent_2)\n",
    "mix_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_inp__before_enc = copy.deepcopy(inp1)\n",
    "mix_inp__before_enc['motion_name_enc'] = mix_latent[None, ...]  # add batch dim\n",
    "\n",
    "op = model(mix_inp__before_enc)\n",
    "np.save(\"./_expts/mix_op__before_enc.npy\", op)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "73ef9dae1850d4826a6a2abacc613ceaa38b06453232522e74b0afa9475d9130"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('ai-choreo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
